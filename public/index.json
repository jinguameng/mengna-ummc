[{"authors":["admin"],"categories":null,"content":"","date":1554595200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1554595200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"","tags":null,"title":"Mengna Zhang","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":[],"categories":[],"content":" Question 1 Let X=the number of shared IBD alleles (Score: 15)\na. What is the distribution of X for a sibling pair and what is the coefficient of relationship and kinship coefficient? (Score 5)\nFor a sibling pair, X could be 0, 1, 2\n\\(f_X(x) = \\begin{cases}\\pi_0,\\ if\\ x = 0\\\\ \\pi_1,\\ if \\ x = 1 \\\\ \\pi_2,\\ if \\ x =2\\end{cases}\\)\nAnd in this case, \\(\\pi_0 = \\frac{1}{4}\\), \\(\\pi_1 = \\frac{1}{2}\\), \\(\\pi_2 = \\frac{1}{4}\\)\nCoeficaient of relationship: \\(E(x) = 0 * \\pi_0 + 1 * \\pi_1 + 2*\\pi_2 \\\\ = 0 + \\frac{1}{2} + \\frac{1}{2} = 1\\)\nKinship coefficient: \\(\\varphi = \\frac{E(X)}{4} = \\frac{1}{4}\\)\nb. What is the distribution of X for a parent-child pair and what is the coefficient of relationship and kinship coefficient? (Score 5)\nThere is only one shared IBD allele between a parent-child pair, and it is also the only situation can happen for a parent-child pair.\n\\(\\pi_0 = 0\\), \\(\\pi_1 = 1\\), \\(\\pi_2 = 2\\)\nCoeficaient of relationship: \\(E(x) = 0 * \\pi_0 + 1 * \\pi_1 + 2*\\pi_2 \\\\ = 0 + 1*1 + 0 = 1\\)\nKinship coefficient: \\(\\varphi = \\frac{E(X)}{4} = \\frac{1}{4}\\)\nc. What is the distribution of X for a grandparent-grandchild pair and what is the coefficient of relationship and kinship coefficient? (Score 5)\nThere must be one shared IBD allele between a parent-child pair, hence, the child must has an allele which is come from his/her grandmother or grandfather.\nSo, \\(\\pi_0 = 0\\), \\(\\pi_1 = \\frac{1}{2}\\), \\(\\pi_2 = 0\\)\nCoeficaient of relationship: \\(E(x) = 0 * \\pi_0 + 1 * \\pi_1 + 2*\\pi_2 \\\\ = 0 + 1*\\frac{1}{2} + 0 = \\frac{1}{2}\\)\nKinship coefficient: \\(\\varphi = \\frac{E(X)}{4} = \\frac{1}{8}\\)\n Question 2 Suppose a disease has autosome dominant inheritance of allele D for a gene locus with allele D and d (i.e. Pr(Y=1|G)=1 if G=DD or Dd, and Pr(Y=1|G)=0 if G=dd). Let X=the number of shared IBD alleles between an affected sibling pair. (Score: 15)\na. If father genotype is Dd and mother genotype is dd, what is probability of X=0, 1 and 2 respectively? (Score: 5)\nChildren’s possible genotypes:\n\\(D_fd_f\\ \\times\\ d_{m1}d_{m2} =\u0026gt; \\begin{cases}D_fd_{m1},\\ Pr = \\frac{1}{4} \\\\ D_fd_{m2},\\ Pr = \\frac{1}{4} \\\\d_fd_{m1},\\ Pr = \\frac{1}{4} \\\\d_fd_{m2},\\ Pr = \\frac{1}{4} \\end{cases}\\)\nAnd among those four cases, only \\(D_fd_{m1}\\) and \\(D_fd_{m2}\\) are affected genotype.\n\\(Pr(D_fd_{m1}) = Pr(D_fd_{m2}) = \\frac{\\frac{1}{4}}{\\frac{1}{4} + \\frac{1}{4}} = \\frac{1}{2}\\)\nBy multiplication rule we can calculate the frequency for each X :\n\\((\\frac{1}{2}D_fd_{m1} + \\frac{1}{2}D_fd_{m2})^2\\)\n\\(= \\frac{1}{4}(D_fd_{m1}, D_fd_{m1}) + \\frac{1}{4}(D_fd_{m2}, D_fd_{m2}) + \\frac{1}{2}(D_fd_{m1}, D_fd_{m2})\\)\n  X Offerspring Genotypes Probability    1 \\((D_fd_{m1}, D_fd_{m2})\\) \\(\\frac{1}{2}\\)  2 \\((D_fd_{m1}, D_fd_{m1})\\), \\((D_fd_{m2}, D_fd_{m2})\\) \\(\\frac{1}{2}\\)    So,\n\\(Pr(x) = \\begin{cases}0,\\ x = 0 \\\\ \\frac{1}{2},\\ x = 1 \\\\ \\frac{1}{2},\\ x = 2\\end{cases}\\)\nb. If father genotype is Dd and mother genotype is also Dd, what is the distribution of X? what is E(X)? and what is the Var(X)? (Score: 10)\nChildren’s possible genotypes:\n\\(D_fd_f\\ \\times\\ D_{m}d_{m} =\u0026gt; \\begin{cases}D_fD_{m},\\ Pr = \\frac{1}{4} \\\\ D_fd_{m},\\ Pr = \\frac{1}{4} \\\\d_fD_{m},\\ Pr = \\frac{1}{4} \\\\d_fd_{m},\\ Pr = \\frac{1}{4} \\end{cases}\\)\nAnd among those four cases, there are three affected genotypes:\n\\(Pr(D_fD_{m}) = Pr(D_fd_{m}) = Pr(d_fD_{m}) = \\frac{1/4}{1/4+1/4 +1/4} = \\frac{1}{3}\\)\nBy multiplication rule we can calculate the frequency for each X :\n\\((\\frac{1}{3}D_fD_m + \\frac{1}{3}D_fd_m + \\frac{1}{3}d_fD_m)^2\\)\n\\(= \\frac{1}{9}(D_fD_m,D_fD_m) + \\frac{1}{9}(D_fd_m,D_fd_m) + \\frac{1}{9}(d_fD_m,d_fD_m) + \\frac{2}{9}(D_fD_m,D_fd_m) + \\frac{2}{9}(D_fD_m,d_fD_m) +\\frac{2}{9}(D_fd_m,d_fD_m)\\)\n  X Offerspring Genotypes Probability    0 \\((D_fd_m,d_fD_m)\\) \\(\\frac{2}{9}\\)  1 \\((D_fD_m,D_fd_m)\\), \\((D_fD_m,d_fD_m)\\) \\(\\frac{4}{9}\\)  2 \\((D_fD_m,D_fD_m)\\), \\((D_fd_m,D_fd_m)\\), \\((d_fD_m,d_fD_m)\\) \\(\\frac{1}{3}\\)    So,\n\\(Pr(x) = \\begin{cases}\\frac{2}{9},\\ x = 0 \\\\ \\frac{4}{9},\\ x = 1 \\\\ \\frac{1}{3},\\ x = 2\\end{cases}\\)\n\\(E(X) = 0*\\frac{2}{9} + 1*\\frac{4}{9} + 2*\\frac{1}{3} = \\frac{10}{9}\\)\n\\(E(X^2) = 0*\\frac{2}{9} + 1*\\frac{4}{9} + 4*\\frac{1}{3} = \\frac{16}{9}\\)\n\\(Var(X) = E(X^2) - E(x)^2 = \\frac{16}{9} - (\\frac{10}{9})^2 = \\frac{52}{27}\\)\n Question 3 Let X be the number of shared IBS alleles for a sibling pair with Dd genotype for both parents. What are the E(X) (Score 10) and Var(X)? (Score 15) \nChildren’s possible genotypes:\n\\[Dd\\ \\times\\ Dd =\u0026gt; \\begin{cases}DD,\\ Pr = \\frac{1}{4} \\\\ Dd,\\ Pr= \\frac{1}{2} \\\\ dd,\\ Pr = \\frac{1}{4} \\end{cases}\\]\nBy multiplication rule we can calculate the frequency for each X:\n\\((\\frac{1}{4}DD + \\frac{1}{2}Dd + \\frac{1}{4}dd)^2\\)\n\\(= \\frac{1}{16}(DD,DD) + \\frac{1}{4}(Dd,Dd) + \\frac{1}{16}(dd,dd) + \\frac{1}{4}(DD,Dd) + \\frac{1}{8}(DD,dd) + \\frac{1}{4}(Dd,dd)\\)\n  X Offerspring Genotypes Probability    0 \\((DD,dd)\\) \\(\\frac{1}{8}\\)  1 \\((DD,Dd)\\), \\((Dd,dd)\\) \\(\\frac{1}{2}\\)  2 \\((DD,DD)\\), \\((Dd,Dd)\\), \\((dd,dd)\\) \\(\\frac{3}{8}\\)    So,\n\\(Pr(x) = \\begin{cases}\\frac{1}{8},\\ x = 0 \\\\ \\frac{1}{2},\\ x = 1 \\\\ \\frac{3}{8},\\ x = 2\\end{cases}\\)\n\\(E(X) = 0*\\frac{1}{8} + 1*\\frac{1}{2} + 2*\\frac{3}{8} = \\frac{5}{4}\\)\n\\(E(X^2) = 0*\\frac{1}{8} + 1*\\frac{1}{2} + 4*\\frac{3}{8} = 2\\)\n\\(Var(X) = E(X^2) - E(X)^2 = 2 - (\\frac{5}{4})^2 = \\frac{7}{16}\\)\n Question 4 a. Fill out the table below. X is a random variable for the number of shared IBS (Score: 10)\nb. For a sibling pair, if pa= 0.5, what is Pr(X=0), Pr(X=1) and Pr(X=2), respectively? What is the E(X) and Var(X)? What are the answers if pa= 0.01. (Score: 5)\nIf \\(p_a = 0.5\\)\n\\(Pr(X=0) = 0.5 * 0.5^2 * 0.5^2 = 0.5^5 = 0.03125\\)\n\\(Pr(X=1) = 2 * 0.5^4 + 2 *0.5 ^4 + 2 *0.5 ^4 = 0.375\\)\n\\(Pr(X=2) = 0.5^4 + 2*0.5^4 + 3.5*0.5^4 + 2*0.4^4 + 0.5^4 = 0.59375\\)\n\\(E(X) = 0*0.03125 + 1*0.375 + 2*0.59375 = 1.5625\\)\n\\(E(X^2) = 0*0.03125 + 1*0.375 + 4*0.59375 = 2.75\\)\n\\(Var(X) = E(X^2) - E(X)^2 = 2.75 - 1.5625^2 = 0.30859375\\)\nIf \\(p_a = 0.01\\)\n\\(Pr(X=0) = 0.5 * 0.01^2 * 0.99^2 = 0.000049005\\)\n\\(Pr(X=1) = 2 * 0.01^3 * 0.99 + 2 *0.01 * 0.99^3 + 2 *0.01^2 * 0.99^2 = 0.01960398\\)\n\\(Pr(X=2) = 0.01^4 + 2*0.01^3 * 0.99 + 3.5*0.01^2 * 0.99^2 + 2*0.01 * 0.99^3 + 0.99^4 = 0.980347015\\)\n Question 5 Causal locus (D and d), LD locus(A and a), f(D) = 0.4, f(A|D) = 0.9, f(A|d) = 0.2. What is the frequency of haplotype DA, haplotype Da, haplotype dA, haplotype da and genotype DdAa in the population? (Score: 10)\nf(D) = 0.4, f(d) = 0.6\nf(A|D) = 0.9, f(a|D) = 0.1\nf(A|d) = 0.2, f(a|d) = 0.8\nPr(DA) = f(A|D)f(D) = 0.9 * 0.4 = 0.36\nPr(Da) = f(a|D)f(D) = 0.1 * 0.4 = 0.04\nPr(dA) = f(A|d)f(d) = 0.2 * 0.6 = 0.12\nPr(da) = f(a|d)f(d) = 0.8 * 0.6 = 0.48\nGenotype DdAa has two possible haplotype combinations: DA-da and Da-dA.\nBased on multiplication rules, we can get:\nPr(DdAa) = 2 * 0.36 * 0.48 + 2 * 0.04 * 0.12 = 0.3552\n Question 6 Causal locus (D and d), LD locus( A and a), f(D) = 0.4, f(A|D) = 0.9, f (A|d) = 0.2. Genotype variable X : the number of D and genotype variable Y : the number of A. What is the covariance of X and Y ? and What is the Correlation? (Score 10) \nf(D) = 0.4, f(d) = 0.6\nf(A|D) = 0.9, f(a|D) = 0.1\nf(A|d) = 0.2, f(a|d) = 0.8\nPr(DA) = f(A|D)f(D) = 0.9 * 0.4 = 0.36\nPr(Da) = f(a|D)f(D) = 0.1 * 0.4 = 0.04\nPr(dA) = f(A|d)f(d) = 0.2 * 0.6 = 0.12\nPr(da) = f(a|d)f(d) = 0.8 * 0.6 = 0.48\nFrom multiplcation rule:\n\\((0.36DA + 0.04Da + 0.12dA + 0.48da)^2\\)\n\\(= 0.1296(DDAA) + 0.0016(DDaa) + 0.0144(ddAA) + 0.2304(ddaa) + 0.0288(DDAa) + 0.0864(DdAA) + 0.3456(DdAa) + 0.0096(DdAa) + 0.0384(Ddaa) + 0.1152(ddAa)\\)\nFor variable X(the number of D):\n  X Genotype Probablity    0 ddAA, ddaa, ddAa 0.36  1 DdAA, DdAa, Ddaa 0.48  2 DDAA, DDaa, DDAa 0.16    E(X) = 0.48 + 0.16*2 = 0.8\n\\(E(X^2) = 0.48 + 0.16*4 = 1.12\\)\n\\(Var(X) = E(X^2) - E(X)^2 = 1.12 - 0.8^2 = 0.48\\)\n\\(sd(X) = \\sqrt{Var(X)} = 0.69282\\)\nFor variable Y(the number of A):\n  Y Genotype Probablity    0 aaDD, aaDd, aadd 0.2704  1 AaDD, AaDd, Aadd 0.4992  2 AADD, AADd, AAdd 0.2304    E(Y) = 0.4992 + 0.2304*2 = 0.96\n\\(E(Y^2) = 0.4992 + 0.2304*4 = 1.4208\\)\n\\(Var(Y) = E(Y^2) - E(Y)^2 = 1.4208 - 0.96^2 = 0.4992\\)\n\\(sd(Y) = \\sqrt{Var(Y)} = 0.70654\\)\nFor variable (XY):\n  XY Genotype Probablity    0 DDaa, ddAA, ddaa, Ddaa, ddAa 0.4  1 DdAa 0.3552  2 DDAa, DdAA 0.1152  4 DDAA 0.1296    E(XY) = \\(0.3552 + 0.1152*2 + 0.1296*4\\) = 1.104\nCov(X,Y) = E(XY) - E(X)E(Y) = 1.104 - 0.8*0.96 = 0.336\nCorr(X,Y) = \\(\\frac{Cov(X,Y)}{sd(X)sd(Y)} = \\frac{0.336}{0.69282*0.70654} = 0.6864\\)\nThe covariance of X and Y is 0.336 and the Correlation is 0.6864.\n Question 7 Causal locus (D and d), LD locus (A and a), f(D) = 0.4, f(A|D) = 0.9, f(A|d) = 0.2. Dichotomous allele variable X: the number of D and dichotomous allele variable Y: the number of A. What is the covariance of X and Y ? (i.e. LD measure δ) and What is the Correlation? (i.e. LD measure r) (Score 10)\nf(D) = 0.4, f(d) = 0.6\nf(A|D) = 0.9, f(a|D) = 0.1\nf(A|d) = 0.2, f(a|d) = 0.8\nVariable X:\nX = 1: f(D) = 0.4\nX = 0: f(d) = 0.6\nE(X) = f(D) = 0.4\nVar(X) = f(D)(1-f(D)) = 0.4*0.6 = 0.24\nVariable Y:\nY = 1: \\(f(A) = f(A|D)f(D) + f(A|d)f(d) = 0.9*0.4 + 0.2*0.6 = 0.48\\)\nY = 0: \\(f(a) = f(a|D)f(D) + f(a|d)f(d) = 0.1*0.4 + 0.8*0.6 = 0.52\\)\nE(Y) = f(A) = 0.48\nVar(Y) = f(A)(1-f(A)) = 0.48*0.52 = 0.2496\nE(XY) = f(DA) = f(A|D)f(D) = 0.9 * 0.4 = 0.36\nCov(X,Y) = E(XY) - E(X)E(Y) = 0.36 - 0.4*0.48 = 0.168\nCorr(X,Y = \\(\\frac{Cov(X,Y)}{\\sqrt{Var(X)}\\sqrt{Var(Y)}} =\\frac{0.168}{\\sqrt{0.24}\\sqrt{0.2496}} = 0.686\\)\nThe covariance of X and Y is 0.168 and the Correlation is 0.686.\n ","date":1586476800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586560193,"objectID":"d59aabed077ec3e94812076919c3ede7","permalink":"/post/bds751hw9/","publishdate":"2020-04-10T00:00:00Z","relpermalink":"/post/bds751hw9/","section":"post","summary":"Question 1 Let X=the number of shared IBD alleles (Score: 15)\na. What is the distribution of X for a sibling pair and what is the coefficient of relationship and kinship coefficient? (Score 5)\nFor a sibling pair, X could be 0, 1, 2\n\\(f_X(x) = \\begin{cases}\\pi_0,\\ if\\ x = 0\\\\ \\pi_1,\\ if \\ x = 1 \\\\ \\pi_2,\\ if \\ x =2\\end{cases}\\)\nAnd in this case, \\(\\pi_0 = \\frac{1}{4}\\), \\(\\pi_1 = \\frac{1}{2}\\), \\(\\pi_2 = \\frac{1}{4}\\)","tags":["bds751","bds751hw"],"title":"Homework 9","type":"post"},{"authors":[],"categories":[],"content":" Question 1 Five students independently collected their own samples and conducted a test of SNP association with hypertension. The test p-values are 0.02, 0.01, 0.03, 0.08, and 0.1. Conduct a meta-analysis and show if this SNP is significantly associated with hypertension (Score: 25) (Hints: combined p-value method) \nMethod: Fisher’s combined probability test.\n\\(p_1 = 0.02\\), \\(p_2 = 0.01\\), \\(p_3 = 0.03\\), \\(p_4 = 0.08\\), \\(p_5 = 0.1\\)\n\\(H_0\\): All of the separate null hypothesis(SNP is significantly with hypertension) is true.\n\\(H_a\\): At least one of the separate alternative hypothesis(SNP is not significantly associated with hypertension) is true.\n\\(S_F = -2\\sum_{i=1}^{k}ln(p_i)\\)\n\\(= -2(ln(p_1) + ln(p_2) + ln(p_3) + ln(p_4) + ln(p_5))\\)\n\\(= -2(ln(0.02) + ln(0.01) + ln(0.03) + ln(0.08) + ln(0.1))\\)\n\\(= -2 * (-16.852)\\)\n\\(= 33.704\\)\n\\(S_F \\sim X^2_{2k}, k = 5\\), \\(S_F\\) follows chi-squared distribution with 10 degree of freedom, and the p-value is 0.000207 which is less than 0.05, so, we reject the null hypothesis and conclude that at least one of the test shows that SNP is not significantly associated with hypertension.\n Question 2 A locus of alleles A and a has frequency, f (A) = 0.3. If our sample contains 100 alleles, (Score: 45) \na. What is the distribution of the number of allele A (NA) in the sample? and write down the probability mass function (Score: 15) \nThe number of allele A is following binomial distribution.\nLet X = the number of allele A\n\\(f(X = x | p = 0.3, n = 100) = \\left(\\begin{matrix}100\\\\x\\end{matrix}\\right)0.3^x0.7^{100-x}\\)\nb. What is the expected number of NA? (Score: 15) \n\\(E(NA) = n(1-p) = 100 *0.7 = 70\\)\nThe expected number of NA is 70.\nc. What is the variance of NA? (Score: 15) \n\\(Var(NA) = np(1-p) = 100 * 0.3 * 0.7 = 21\\)\nThe variance of NA is 21.\n Question 3 If a random variable X follows uniform distribution, X~U(-1,1), and Y=X^2, what is the covariance of X and Y? are X and Y independent? (Score: 10) \n\\(Cov(X,Y) = E(XY) - E(X)E(Y)\\)\n\\(= E(X^3) - E(X)E(X)\\)\nBecause X follows uniform distribution,\n=\u0026gt; \\(E(X) = \\frac{-1+1}{2} = 0\\).\n=\u0026gt; \\(E(X)E(Y) = 0\\)\n\\(E(X^3) = \\int_{-1}^{1}x^3dx = \\frac{1}{4}x^4|_{-1}^{1} = \\frac{1}{4} - \\frac{1}{4} = 0\\)\nSo, \\(Cov(X,Y) = 0 - 0 = 0\\), the covariance of X and Y is 0, but X and Y are not independent, since \\(Y = X^2\\)\n Question 4 For multinomial distribution below, show that cov(xi , xj) = −npipj. (Score 20) \n\\(f\\left(x_1,x_2,\\ldots x_n;n,p_1,p_2,\\ldots,p_n\\right)=\\left(\\begin{matrix}n\\\\x_1,x_2,\\ldots x_n\\\\\\end{matrix}\\right)p_1^{x1}p_2^{x2}\\ldots p_n^{xn}\\)\nBased on theorem, we know that \\(x_i\\) and \\(x_j\\) are following Binomial distribution:\n\\(x_i \\sim Binomial(n,p_i)\\), \\(x_j \\sim Binomial(n,p_j)\\).\n\\(E(x_i) = np_i\\), \\(Var(x_i) = np_i(1-p_i)\\)\n\\(E(x_j) = np_j\\), \\(Var(x_j) = np_j(1-p_j)\\)\nAnd \\((x_i+x_j)\\) also following binomial distribution.\n\\((x_i + x_j) \\sim Binomial(n, p_i + p_j)\\),\n\\(E(x_i+x_j) = n(p_i+p_j)\\)\n\\(Var(x_i+x_j) = n(p_i+p_j)(1-p_i-p_j)\\)\n\\(Cov(x_i,x_j) = \\frac{1}{2}[Var(x_i + x_j)-Var(x_i)-Var(x_j)]\\)\n\\(= \\frac{1}{2}[n(p_i+p_j)(1-p_i-p_j) -np_i(1-p_i) - np_j(1-p_j)]\\)\n\\(= \\frac{1}{2}(-2np_ip_j)\\)\n\\(= -np_ip_j\\)\n ","date":1585958400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586050087,"objectID":"a97453a73ffbab5c17c5c344faf9f3ed","permalink":"/post/bds751hw8/","publishdate":"2020-04-04T00:00:00Z","relpermalink":"/post/bds751hw8/","section":"post","summary":"Question 1 Five students independently collected their own samples and conducted a test of SNP association with hypertension. The test p-values are 0.02, 0.01, 0.03, 0.08, and 0.1. Conduct a meta-analysis and show if this SNP is significantly associated with hypertension (Score: 25) (Hints: combined p-value method) \nMethod: Fisher’s combined probability test.\n\\(p_1 = 0.02\\), \\(p_2 = 0.01\\), \\(p_3 = 0.03\\), \\(p_4 = 0.08\\), \\(p_5 = 0.1\\)\n\\(H_0\\): All of the separate null hypothesis(SNP is significantly with hypertension) is true.","tags":["bds751","bds751hw"],"title":"Homework 8","type":"post"},{"authors":[],"categories":[],"content":" Question 1 For a cell in meiosis interphase, how many chromosomes and chromatids are there in G1? and how many in G2? (Score: 5)\nG1: 46 chromosomes, 0 chromatids.\nG2: 46 chromosomes, 92 chromatids.\n Question 2 How many daughter cells will be for a cell after meiosis I? and how many chromosomes and chromatids are there for each daughter cell? (Score: 5)\n2 daughter cells after meiosis I.\n23 chromosomes and 46 chromotids.\n Question 3 How many daughter cells will be for a cell after meiosis II? And how many chromosomes and chromatids are there for each daughter cell? (Score: 5)\nFour daughter cells.\n23 chromosomes and 0 chromotids.\n Question 4 Recombination can be seen as chiasmata between homologous chromosomes at what stage of cell division? (Score: 5)\nMetaphase I at Meiosis I.\n Question 5 Is it possible for meiosis to occur in a haploid cell? (Score: 5) Extra credit: Is mitosis possible? (Score: 5)\nNo. Meiosis requires even number of chromosomes. And Mitosis could happen in a haploid cell.\n Question 6 For two loci with genotypes of Aa and Bb, which one of the following could happen: segregation without independent assortment; or independent assortment without segregation? (Score: 5)\nSegregation without independent assortment could happen.\n Question 7 If an individual has alleles of A and a in a locus, what types of gamete can be made? and what is the probability for each type of gamete? (Score: 5) \nTwo possiable gametes: A and a.\nPr(A) = 0.5 and Pr(a) = 0.5\n Question 8 If an individual has alleles of A and a at a locus of chr 1, but only allele B at a locus of chr 2, what types of gamete can be made? and what is the probability for each type of gamete? (Score: 5)\nTwo possiable gametes: AB and aB.\nPr(AB) = 0.5 and Pr(aB) = 0.5\n Question 9 If an individual has alleles of A and a at a locus of chr 1, and alleles of B and b at a locus of chr 2, what types of gamete can be made? and what is the probability for each type of gamete? (Score: 5) \nFour possiable gametes: AB, aB, Ab, ab.\nPr(AB) = Pr(aB) = Pr(Ab) = Pr(ab) = 0.25\n Question 10 How many different kinds of gametes can be produced by an individual who carries heterozygotic alleles at three loci of different chromosomes (e.g. Aa of locus 1 at chr 1, Bb of locus 2 at chr 2 and Cc of locus 3 at chr 3)? (Score: 5) \n\\(2^3 = 8\\)\n Question 11 If an individual has heterozygous genotypes at each of 6 loci, how many types of gametes can be produced under assumption of independent assortment? (Score: 5) \n\\(2^6 = 64\\)\n Question 12 If both parents have genotypes of Aa, what genotypes can be observed for their kid and what their probabilities are? (Score: 5)\nGenotypes: AA. Aa, aa\nPr(AA) = Pr(aa) = 0.25 and Pr(Aa) = 0.5\n Question 13 Father has genotype of AaBB and mother has genotype of Aabb. With independent assortment for all loci, what genotypes can be observed for their kid and what their probabilities? (Score: 5) \n Question 14  ## Question 15  ## Question 16  ## Question 17  ## Question 18 \n ","date":1584302552,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584302552,"objectID":"43118030d4910aa9d40306cc9652be7f","permalink":"/post/bds723hw4/","publishdate":"2020-03-15T15:02:32-05:00","relpermalink":"/post/bds723hw4/","section":"post","summary":"Question 1 For a cell in meiosis interphase, how many chromosomes and chromatids are there in G1? and how many in G2? (Score: 5)\nG1: 46 chromosomes, 0 chromatids.\nG2: 46 chromosomes, 92 chromatids.\n Question 2 How many daughter cells will be for a cell after meiosis I? and how many chromosomes and chromatids are there for each daughter cell? (Score: 5)\n2 daughter cells after meiosis I.","tags":["bds751","bds751hw"],"title":"Homework 4","type":"post"},{"authors":[],"categories":[],"content":" knitr::opts_chunk$set(echo = TRUE) library(tidyverse) ## ── Attaching packages ──── tidyverse 1.3.0 ── ## ✓ ggplot2 3.2.1 ✓ purrr 0.3.3 ## ✓ tibble 2.1.3 ✓ dplyr 0.8.3 ## ✓ tidyr 1.0.2 ✓ stringr 1.4.0 ## ✓ readr 1.3.1 ✓ forcats 0.4.0 ## ── Conflicts ─────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() library(tidyr) Dataset: WHO mydata \u0026lt;- who mydata ## # A tibble: 7,240 x 60 ## country iso2 iso3 year new_sp_m014 new_sp_m1524 new_sp_m2534 new_sp_m3544 ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; ## 1 Afghan… AF AFG 1980 NA NA NA NA ## 2 Afghan… AF AFG 1981 NA NA NA NA ## 3 Afghan… AF AFG 1982 NA NA NA NA ## 4 Afghan… AF AFG 1983 NA NA NA NA ## 5 Afghan… AF AFG 1984 NA NA NA NA ## 6 Afghan… AF AFG 1985 NA NA NA NA ## 7 Afghan… AF AFG 1986 NA NA NA NA ## 8 Afghan… AF AFG 1987 NA NA NA NA ## 9 Afghan… AF AFG 1988 NA NA NA NA ## 10 Afghan… AF AFG 1989 NA NA NA NA ## # … with 7,230 more rows, and 52 more variables: new_sp_m4554 \u0026lt;int\u0026gt;, ## # new_sp_m5564 \u0026lt;int\u0026gt;, new_sp_m65 \u0026lt;int\u0026gt;, new_sp_f014 \u0026lt;int\u0026gt;, ## # new_sp_f1524 \u0026lt;int\u0026gt;, new_sp_f2534 \u0026lt;int\u0026gt;, new_sp_f3544 \u0026lt;int\u0026gt;, ## # new_sp_f4554 \u0026lt;int\u0026gt;, new_sp_f5564 \u0026lt;int\u0026gt;, new_sp_f65 \u0026lt;int\u0026gt;, ## # new_sn_m014 \u0026lt;int\u0026gt;, new_sn_m1524 \u0026lt;int\u0026gt;, new_sn_m2534 \u0026lt;int\u0026gt;, ## # new_sn_m3544 \u0026lt;int\u0026gt;, new_sn_m4554 \u0026lt;int\u0026gt;, new_sn_m5564 \u0026lt;int\u0026gt;, ## # new_sn_m65 \u0026lt;int\u0026gt;, new_sn_f014 \u0026lt;int\u0026gt;, new_sn_f1524 \u0026lt;int\u0026gt;, ## # new_sn_f2534 \u0026lt;int\u0026gt;, new_sn_f3544 \u0026lt;int\u0026gt;, new_sn_f4554 \u0026lt;int\u0026gt;, ## # new_sn_f5564 \u0026lt;int\u0026gt;, new_sn_f65 \u0026lt;int\u0026gt;, new_ep_m014 \u0026lt;int\u0026gt;, ## # new_ep_m1524 \u0026lt;int\u0026gt;, new_ep_m2534 \u0026lt;int\u0026gt;, new_ep_m3544 \u0026lt;int\u0026gt;, ## # new_ep_m4554 \u0026lt;int\u0026gt;, new_ep_m5564 \u0026lt;int\u0026gt;, new_ep_m65 \u0026lt;int\u0026gt;, ## # new_ep_f014 \u0026lt;int\u0026gt;, new_ep_f1524 \u0026lt;int\u0026gt;, new_ep_f2534 \u0026lt;int\u0026gt;, ## # new_ep_f3544 \u0026lt;int\u0026gt;, new_ep_f4554 \u0026lt;int\u0026gt;, new_ep_f5564 \u0026lt;int\u0026gt;, ## # new_ep_f65 \u0026lt;int\u0026gt;, newrel_m014 \u0026lt;int\u0026gt;, newrel_m1524 \u0026lt;int\u0026gt;, ## # newrel_m2534 \u0026lt;int\u0026gt;, newrel_m3544 \u0026lt;int\u0026gt;, newrel_m4554 \u0026lt;int\u0026gt;, ## # newrel_m5564 \u0026lt;int\u0026gt;, newrel_m65 \u0026lt;int\u0026gt;, newrel_f014 \u0026lt;int\u0026gt;, ## # newrel_f1524 \u0026lt;int\u0026gt;, newrel_f2534 \u0026lt;int\u0026gt;, newrel_f3544 \u0026lt;int\u0026gt;, ## # newrel_f4554 \u0026lt;int\u0026gt;, newrel_f5564 \u0026lt;int\u0026gt;, newrel_f65 \u0026lt;int\u0026gt; glimpse(mydata) ## Observations: 7,240 ## Variables: 60 ## $ country \u0026lt;chr\u0026gt; \u0026quot;Afghanistan\u0026quot;, \u0026quot;Afghanistan\u0026quot;, \u0026quot;Afghanistan\u0026quot;, \u0026quot;Afghanista… ## $ iso2 \u0026lt;chr\u0026gt; \u0026quot;AF\u0026quot;, \u0026quot;AF\u0026quot;, \u0026quot;AF\u0026quot;, \u0026quot;AF\u0026quot;, \u0026quot;AF\u0026quot;, \u0026quot;AF\u0026quot;, \u0026quot;AF\u0026quot;, \u0026quot;AF\u0026quot;, \u0026quot;AF\u0026quot;, \u0026quot;A… ## $ iso3 \u0026lt;chr\u0026gt; \u0026quot;AFG\u0026quot;, \u0026quot;AFG\u0026quot;, \u0026quot;AFG\u0026quot;, \u0026quot;AFG\u0026quot;, \u0026quot;AFG\u0026quot;, \u0026quot;AFG\u0026quot;, \u0026quot;AFG\u0026quot;, \u0026quot;AFG\u0026quot;, … ## $ year \u0026lt;int\u0026gt; 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 19… ## $ new_sp_m014 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_sp_m1524 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_sp_m2534 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_sp_m3544 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_sp_m4554 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_sp_m5564 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_sp_m65 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_sp_f014 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_sp_f1524 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_sp_f2534 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_sp_f3544 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_sp_f4554 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_sp_f5564 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_sp_f65 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_sn_m014 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_sn_m1524 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_sn_m2534 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_sn_m3544 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_sn_m4554 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_sn_m5564 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_sn_m65 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_sn_f014 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_sn_f1524 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_sn_f2534 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_sn_f3544 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_sn_f4554 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_sn_f5564 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_sn_f65 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_ep_m014 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_ep_m1524 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_ep_m2534 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_ep_m3544 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_ep_m4554 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_ep_m5564 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_ep_m65 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_ep_f014 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_ep_f1524 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_ep_f2534 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_ep_f3544 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_ep_f4554 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_ep_f5564 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_ep_f65 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ newrel_m014 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ newrel_m1524 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ newrel_m2534 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ newrel_m3544 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ newrel_m4554 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ newrel_m5564 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ newrel_m65 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ newrel_f014 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ newrel_f1524 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ newrel_f2534 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ newrel_f3544 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ newrel_f4554 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ newrel_f5564 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ newrel_f65 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … colnames(mydata) ## [1] \u0026quot;country\u0026quot; \u0026quot;iso2\u0026quot; \u0026quot;iso3\u0026quot; \u0026quot;year\u0026quot; \u0026quot;new_sp_m014\u0026quot; ## [6] \u0026quot;new_sp_m1524\u0026quot; \u0026quot;new_sp_m2534\u0026quot; \u0026quot;new_sp_m3544\u0026quot; \u0026quot;new_sp_m4554\u0026quot; \u0026quot;new_sp_m5564\u0026quot; ## [11] \u0026quot;new_sp_m65\u0026quot; \u0026quot;new_sp_f014\u0026quot; \u0026quot;new_sp_f1524\u0026quot; \u0026quot;new_sp_f2534\u0026quot; \u0026quot;new_sp_f3544\u0026quot; ## [16] \u0026quot;new_sp_f4554\u0026quot; \u0026quot;new_sp_f5564\u0026quot; \u0026quot;new_sp_f65\u0026quot; \u0026quot;new_sn_m014\u0026quot; \u0026quot;new_sn_m1524\u0026quot; ## [21] \u0026quot;new_sn_m2534\u0026quot; \u0026quot;new_sn_m3544\u0026quot; \u0026quot;new_sn_m4554\u0026quot; \u0026quot;new_sn_m5564\u0026quot; \u0026quot;new_sn_m65\u0026quot; ## [26] \u0026quot;new_sn_f014\u0026quot; \u0026quot;new_sn_f1524\u0026quot; \u0026quot;new_sn_f2534\u0026quot; \u0026quot;new_sn_f3544\u0026quot; \u0026quot;new_sn_f4554\u0026quot; ## [31] \u0026quot;new_sn_f5564\u0026quot; \u0026quot;new_sn_f65\u0026quot; \u0026quot;new_ep_m014\u0026quot; \u0026quot;new_ep_m1524\u0026quot; \u0026quot;new_ep_m2534\u0026quot; ## [36] \u0026quot;new_ep_m3544\u0026quot; \u0026quot;new_ep_m4554\u0026quot; \u0026quot;new_ep_m5564\u0026quot; \u0026quot;new_ep_m65\u0026quot; \u0026quot;new_ep_f014\u0026quot; ## [41] \u0026quot;new_ep_f1524\u0026quot; \u0026quot;new_ep_f2534\u0026quot; \u0026quot;new_ep_f3544\u0026quot; \u0026quot;new_ep_f4554\u0026quot; \u0026quot;new_ep_f5564\u0026quot; ## [46] \u0026quot;new_ep_f65\u0026quot; \u0026quot;newrel_m014\u0026quot; \u0026quot;newrel_m1524\u0026quot; \u0026quot;newrel_m2534\u0026quot; \u0026quot;newrel_m3544\u0026quot; ## [51] \u0026quot;newrel_m4554\u0026quot; \u0026quot;newrel_m5564\u0026quot; \u0026quot;newrel_m65\u0026quot; \u0026quot;newrel_f014\u0026quot; \u0026quot;newrel_f1524\u0026quot; ## [56] \u0026quot;newrel_f2534\u0026quot; \u0026quot;newrel_f3544\u0026quot; \u0026quot;newrel_f4554\u0026quot; \u0026quot;newrel_f5564\u0026quot; \u0026quot;newrel_f65\u0026quot; # combine all the TB cases columns mydata\u0026lt;-mydata %\u0026gt;% pivot_longer(cols = new_sp_m014:newrel_f65,names_to = \u0026quot;cases\u0026quot;,values_to = \u0026quot;value\u0026quot;) mydata ## # A tibble: 405,440 x 6 ## country iso2 iso3 year cases value ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 Afghanistan AF AFG 1980 new_sp_m014 NA ## 2 Afghanistan AF AFG 1980 new_sp_m1524 NA ## 3 Afghanistan AF AFG 1980 new_sp_m2534 NA ## 4 Afghanistan AF AFG 1980 new_sp_m3544 NA ## 5 Afghanistan AF AFG 1980 new_sp_m4554 NA ## 6 Afghanistan AF AFG 1980 new_sp_m5564 NA ## 7 Afghanistan AF AFG 1980 new_sp_m65 NA ## 8 Afghanistan AF AFG 1980 new_sp_f014 NA ## 9 Afghanistan AF AFG 1980 new_sp_f1524 NA ## 10 Afghanistan AF AFG 1980 new_sp_f2534 NA ## # … with 405,430 more rows # change \u0026quot;newrel\u0026quot; to \u0026quot;new_rel\u0026quot; to match the pattern mydata \u0026lt;- mydata %\u0026gt;% mutate(cases = str_replace(cases, pattern = \u0026quot;newr\u0026quot;, replacement = \u0026quot;new_r\u0026quot;)) %\u0026gt;% separate(col = cases, into=c(\u0026quot;new\u0026quot;,\u0026quot;diagnosis\u0026quot;,\u0026quot;gender\u0026quot;), sep=\u0026quot;_\u0026quot;) mydata ## # A tibble: 405,440 x 8 ## country iso2 iso3 year new diagnosis gender value ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 Afghanistan AF AFG 1980 new sp m014 NA ## 2 Afghanistan AF AFG 1980 new sp m1524 NA ## 3 Afghanistan AF AFG 1980 new sp m2534 NA ## 4 Afghanistan AF AFG 1980 new sp m3544 NA ## 5 Afghanistan AF AFG 1980 new sp m4554 NA ## 6 Afghanistan AF AFG 1980 new sp m5564 NA ## 7 Afghanistan AF AFG 1980 new sp m65 NA ## 8 Afghanistan AF AFG 1980 new sp f014 NA ## 9 Afghanistan AF AFG 1980 new sp f1524 NA ## 10 Afghanistan AF AFG 1980 new sp f2534 NA ## # … with 405,430 more rows mydata\u0026lt;-mydata %\u0026gt;% separate(col=gender,into= c(\u0026quot;gender\u0026quot;,\u0026quot;age\u0026quot;),sep = 1) mydata ## # A tibble: 405,440 x 9 ## country iso2 iso3 year new diagnosis gender age value ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 Afghanistan AF AFG 1980 new sp m 014 NA ## 2 Afghanistan AF AFG 1980 new sp m 1524 NA ## 3 Afghanistan AF AFG 1980 new sp m 2534 NA ## 4 Afghanistan AF AFG 1980 new sp m 3544 NA ## 5 Afghanistan AF AFG 1980 new sp m 4554 NA ## 6 Afghanistan AF AFG 1980 new sp m 5564 NA ## 7 Afghanistan AF AFG 1980 new sp m 65 NA ## 8 Afghanistan AF AFG 1980 new sp f 014 NA ## 9 Afghanistan AF AFG 1980 new sp f 1524 NA ## 10 Afghanistan AF AFG 1980 new sp f 2534 NA ## # … with 405,430 more rows # change age to factor var mydata$age %\u0026gt;% unique() ## [1] \u0026quot;014\u0026quot; \u0026quot;1524\u0026quot; \u0026quot;2534\u0026quot; \u0026quot;3544\u0026quot; \u0026quot;4554\u0026quot; \u0026quot;5564\u0026quot; \u0026quot;65\u0026quot; mydata\u0026lt;-mydata %\u0026gt;% mutate( age = recode_factor( age, \u0026quot;014\u0026quot; = \u0026quot;0-14\u0026quot;, \u0026quot;1524\u0026quot; = \u0026quot;15-24\u0026quot;, \u0026quot;2534\u0026quot; = \u0026quot;25-34\u0026quot;, \u0026quot;3544\u0026quot; = \u0026quot;35-44\u0026quot;, \u0026quot;4554\u0026quot; = \u0026quot;45-54\u0026quot;, \u0026quot;5564\u0026quot; = \u0026quot;55-64\u0026quot;, \u0026quot;65\u0026quot; = \u0026quot;65 and above\u0026quot; ) ) mydata ## # A tibble: 405,440 x 9 ## country iso2 iso3 year new diagnosis gender age value ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; ## 1 Afghanistan AF AFG 1980 new sp m 0-14 NA ## 2 Afghanistan AF AFG 1980 new sp m 15-24 NA ## 3 Afghanistan AF AFG 1980 new sp m 25-34 NA ## 4 Afghanistan AF AFG 1980 new sp m 35-44 NA ## 5 Afghanistan AF AFG 1980 new sp m 45-54 NA ## 6 Afghanistan AF AFG 1980 new sp m 55-64 NA ## 7 Afghanistan AF AFG 1980 new sp m 65 and above NA ## 8 Afghanistan AF AFG 1980 new sp f 0-14 NA ## 9 Afghanistan AF AFG 1980 new sp f 15-24 NA ## 10 Afghanistan AF AFG 1980 new sp f 25-34 NA ## # … with 405,430 more rows # remove new mydata\u0026lt;-mydata %\u0026gt;% select(-new) # factor variable mydata \u0026lt;- mydata %\u0026gt;% mutate(gender = parse_factor(gender, levels = c(\u0026quot;f\u0026quot;,\u0026quot;m\u0026quot;)), diagnosis = parse_factor(diagnosis)) %\u0026gt;% rename(cases = value) mydata ## # A tibble: 405,440 x 8 ## country iso2 iso3 year diagnosis gender age cases ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; ## 1 Afghanistan AF AFG 1980 sp m 0-14 NA ## 2 Afghanistan AF AFG 1980 sp m 15-24 NA ## 3 Afghanistan AF AFG 1980 sp m 25-34 NA ## 4 Afghanistan AF AFG 1980 sp m 35-44 NA ## 5 Afghanistan AF AFG 1980 sp m 45-54 NA ## 6 Afghanistan AF AFG 1980 sp m 55-64 NA ## 7 Afghanistan AF AFG 1980 sp m 65 and above NA ## 8 Afghanistan AF AFG 1980 sp f 0-14 NA ## 9 Afghanistan AF AFG 1980 sp f 15-24 NA ## 10 Afghanistan AF AFG 1980 sp f 25-34 NA ## # … with 405,430 more rows  ","date":1583900394,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583900394,"objectID":"4fffe9d90d6722e82b1617defa638eb5","permalink":"/post/bds723-tidy-data/","publishdate":"2020-03-10T23:19:54-05:00","relpermalink":"/post/bds723-tidy-data/","section":"post","summary":" knitr::opts_chunk$set(echo = TRUE) library(tidyverse) ## ── Attaching packages ──── tidyverse 1.3.0 ── ## ✓ ggplot2 3.2.1 ✓ purrr 0.3.3 ## ✓ tibble 2.1.3 ✓ dplyr 0.8.3 ## ✓ tidyr 1.0.2 ✓ stringr 1.4.0 ## ✓ readr 1.3.1 ✓ forcats 0.4.0 ## ── Conflicts ─────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() library(tidyr) Dataset: WHO mydata \u0026lt;- who mydata ## # A tibble: 7,240 x 60 ## country iso2 iso3 year new_sp_m014 new_sp_m1524 new_sp_m2534 new_sp_m3544 ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; ## 1 Afghan… AF AFG 1980 NA NA NA NA ## 2 Afghan… AF AFG 1981 NA NA NA NA ## 3 Afghan… AF AFG 1982 NA NA NA NA ## 4 Afghan… AF AFG 1983 NA NA NA NA ## 5 Afghan… AF AFG 1984 NA NA NA NA ## 6 Afghan… AF AFG 1985 NA NA NA NA ## 7 Afghan… AF AFG 1986 NA NA NA NA ## 8 Afghan… AF AFG 1987 NA NA NA NA ## 9 Afghan… AF AFG 1988 NA NA NA NA ## 10 Afghan… AF AFG 1989 NA NA NA NA ## # … with 7,230 more rows, and 52 more variables: new_sp_m4554 \u0026lt;int\u0026gt;, ## # new_sp_m5564 \u0026lt;int\u0026gt;, new_sp_m65 \u0026lt;int\u0026gt;, new_sp_f014 \u0026lt;int\u0026gt;, ## # new_sp_f1524 \u0026lt;int\u0026gt;, new_sp_f2534 \u0026lt;int\u0026gt;, new_sp_f3544 \u0026lt;int\u0026gt;, ## # new_sp_f4554 \u0026lt;int\u0026gt;, new_sp_f5564 \u0026lt;int\u0026gt;, new_sp_f65 \u0026lt;int\u0026gt;, ## # new_sn_m014 \u0026lt;int\u0026gt;, new_sn_m1524 \u0026lt;int\u0026gt;, new_sn_m2534 \u0026lt;int\u0026gt;, ## # new_sn_m3544 \u0026lt;int\u0026gt;, new_sn_m4554 \u0026lt;int\u0026gt;, new_sn_m5564 \u0026lt;int\u0026gt;, ## # new_sn_m65 \u0026lt;int\u0026gt;, new_sn_f014 \u0026lt;int\u0026gt;, new_sn_f1524 \u0026lt;int\u0026gt;, ## # new_sn_f2534 \u0026lt;int\u0026gt;, new_sn_f3544 \u0026lt;int\u0026gt;, new_sn_f4554 \u0026lt;int\u0026gt;, ## # new_sn_f5564 \u0026lt;int\u0026gt;, new_sn_f65 \u0026lt;int\u0026gt;, new_ep_m014 \u0026lt;int\u0026gt;, ## # new_ep_m1524 \u0026lt;int\u0026gt;, new_ep_m2534 \u0026lt;int\u0026gt;, new_ep_m3544 \u0026lt;int\u0026gt;, ## # new_ep_m4554 \u0026lt;int\u0026gt;, new_ep_m5564 \u0026lt;int\u0026gt;, new_ep_m65 \u0026lt;int\u0026gt;, ## # new_ep_f014 \u0026lt;int\u0026gt;, new_ep_f1524 \u0026lt;int\u0026gt;, new_ep_f2534 \u0026lt;int\u0026gt;, ## # new_ep_f3544 \u0026lt;int\u0026gt;, new_ep_f4554 \u0026lt;int\u0026gt;, new_ep_f5564 \u0026lt;int\u0026gt;, ## # new_ep_f65 \u0026lt;int\u0026gt;, newrel_m014 \u0026lt;int\u0026gt;, newrel_m1524 \u0026lt;int\u0026gt;, ## # newrel_m2534 \u0026lt;int\u0026gt;, newrel_m3544 \u0026lt;int\u0026gt;, newrel_m4554 \u0026lt;int\u0026gt;, ## # newrel_m5564 \u0026lt;int\u0026gt;, newrel_m65 \u0026lt;int\u0026gt;, newrel_f014 \u0026lt;int\u0026gt;, ## # newrel_f1524 \u0026lt;int\u0026gt;, newrel_f2534 \u0026lt;int\u0026gt;, newrel_f3544 \u0026lt;int\u0026gt;, ## # newrel_f4554 \u0026lt;int\u0026gt;, newrel_f5564 \u0026lt;int\u0026gt;, newrel_f65 \u0026lt;int\u0026gt; glimpse(mydata) ## Observations: 7,240 ## Variables: 60 ## $ country \u0026lt;chr\u0026gt; \u0026quot;Afghanistan\u0026quot;, \u0026quot;Afghanistan\u0026quot;, \u0026quot;Afghanistan\u0026quot;, \u0026quot;Afghanista… ## $ iso2 \u0026lt;chr\u0026gt; \u0026quot;AF\u0026quot;, \u0026quot;AF\u0026quot;, \u0026quot;AF\u0026quot;, \u0026quot;AF\u0026quot;, \u0026quot;AF\u0026quot;, \u0026quot;AF\u0026quot;, \u0026quot;AF\u0026quot;, \u0026quot;AF\u0026quot;, \u0026quot;AF\u0026quot;, \u0026quot;A… ## $ iso3 \u0026lt;chr\u0026gt; \u0026quot;AFG\u0026quot;, \u0026quot;AFG\u0026quot;, \u0026quot;AFG\u0026quot;, \u0026quot;AFG\u0026quot;, \u0026quot;AFG\u0026quot;, \u0026quot;AFG\u0026quot;, \u0026quot;AFG\u0026quot;, \u0026quot;AFG\u0026quot;, … ## $ year \u0026lt;int\u0026gt; 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 19… ## $ new_sp_m014 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_sp_m1524 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_sp_m2534 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_sp_m3544 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_sp_m4554 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_sp_m5564 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_sp_m65 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_sp_f014 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_sp_f1524 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_sp_f2534 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_sp_f3544 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_sp_f4554 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_sp_f5564 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_sp_f65 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_sn_m014 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_sn_m1524 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_sn_m2534 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_sn_m3544 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_sn_m4554 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_sn_m5564 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_sn_m65 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_sn_f014 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_sn_f1524 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_sn_f2534 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_sn_f3544 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_sn_f4554 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_sn_f5564 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_sn_f65 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_ep_m014 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_ep_m1524 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_ep_m2534 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_ep_m3544 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_ep_m4554 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_ep_m5564 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_ep_m65 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_ep_f014 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_ep_f1524 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_ep_f2534 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_ep_f3544 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_ep_f4554 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_ep_f5564 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ new_ep_f65 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ newrel_m014 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ newrel_m1524 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ newrel_m2534 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ newrel_m3544 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ newrel_m4554 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ newrel_m5564 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ newrel_m65 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ newrel_f014 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ newrel_f1524 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ newrel_f2534 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ newrel_f3544 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ newrel_f4554 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ newrel_f5564 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ newrel_f65 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … colnames(mydata) ## [1] \u0026quot;country\u0026quot; \u0026quot;iso2\u0026quot; \u0026quot;iso3\u0026quot; \u0026quot;year\u0026quot; \u0026quot;new_sp_m014\u0026quot; ## [6] \u0026quot;new_sp_m1524\u0026quot; \u0026quot;new_sp_m2534\u0026quot; \u0026quot;new_sp_m3544\u0026quot; \u0026quot;new_sp_m4554\u0026quot; \u0026quot;new_sp_m5564\u0026quot; ## [11] \u0026quot;new_sp_m65\u0026quot; \u0026quot;new_sp_f014\u0026quot; \u0026quot;new_sp_f1524\u0026quot; \u0026quot;new_sp_f2534\u0026quot; \u0026quot;new_sp_f3544\u0026quot; ## [16] \u0026quot;new_sp_f4554\u0026quot; \u0026quot;new_sp_f5564\u0026quot; \u0026quot;new_sp_f65\u0026quot; \u0026quot;new_sn_m014\u0026quot; \u0026quot;new_sn_m1524\u0026quot; ## [21] \u0026quot;new_sn_m2534\u0026quot; \u0026quot;new_sn_m3544\u0026quot; \u0026quot;new_sn_m4554\u0026quot; \u0026quot;new_sn_m5564\u0026quot; \u0026quot;new_sn_m65\u0026quot; ## [26] \u0026quot;new_sn_f014\u0026quot; \u0026quot;new_sn_f1524\u0026quot; \u0026quot;new_sn_f2534\u0026quot; \u0026quot;new_sn_f3544\u0026quot; \u0026quot;new_sn_f4554\u0026quot; ## [31] \u0026quot;new_sn_f5564\u0026quot; \u0026quot;new_sn_f65\u0026quot; \u0026quot;new_ep_m014\u0026quot; \u0026quot;new_ep_m1524\u0026quot; \u0026quot;new_ep_m2534\u0026quot; ## [36] \u0026quot;new_ep_m3544\u0026quot; \u0026quot;new_ep_m4554\u0026quot; \u0026quot;new_ep_m5564\u0026quot; \u0026quot;new_ep_m65\u0026quot; \u0026quot;new_ep_f014\u0026quot; ## [41] \u0026quot;new_ep_f1524\u0026quot; \u0026quot;new_ep_f2534\u0026quot; \u0026quot;new_ep_f3544\u0026quot; \u0026quot;new_ep_f4554\u0026quot; \u0026quot;new_ep_f5564\u0026quot; ## [46] \u0026quot;new_ep_f65\u0026quot; \u0026quot;newrel_m014\u0026quot; \u0026quot;newrel_m1524\u0026quot; \u0026quot;newrel_m2534\u0026quot; \u0026quot;newrel_m3544\u0026quot; ## [51] \u0026quot;newrel_m4554\u0026quot; \u0026quot;newrel_m5564\u0026quot; \u0026quot;newrel_m65\u0026quot; \u0026quot;newrel_f014\u0026quot; \u0026quot;newrel_f1524\u0026quot; ## [56] \u0026quot;newrel_f2534\u0026quot; \u0026quot;newrel_f3544\u0026quot; \u0026quot;newrel_f4554\u0026quot; \u0026quot;newrel_f5564\u0026quot; \u0026quot;newrel_f65\u0026quot; # combine all the TB cases columns mydata\u0026lt;-mydata %\u0026gt;% pivot_longer(cols = new_sp_m014:newrel_f65,names_to = \u0026quot;cases\u0026quot;,values_to = \u0026quot;value\u0026quot;) mydata ## # A tibble: 405,440 x 6 ## country iso2 iso3 year cases value ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 Afghanistan AF AFG 1980 new_sp_m014 NA ## 2 Afghanistan AF AFG 1980 new_sp_m1524 NA ## 3 Afghanistan AF AFG 1980 new_sp_m2534 NA ## 4 Afghanistan AF AFG 1980 new_sp_m3544 NA ## 5 Afghanistan AF AFG 1980 new_sp_m4554 NA ## 6 Afghanistan AF AFG 1980 new_sp_m5564 NA ## 7 Afghanistan AF AFG 1980 new_sp_m65 NA ## 8 Afghanistan AF AFG 1980 new_sp_f014 NA ## 9 Afghanistan AF AFG 1980 new_sp_f1524 NA ## 10 Afghanistan AF AFG 1980 new_sp_f2534 NA ## # … with 405,430 more rows # change \u0026quot;newrel\u0026quot; to \u0026quot;new_rel\u0026quot; to match the pattern mydata \u0026lt;- mydata %\u0026gt;% mutate(cases = str_replace(cases, pattern = \u0026quot;newr\u0026quot;, replacement = \u0026quot;new_r\u0026quot;)) %\u0026gt;% separate(col = cases, into=c(\u0026quot;new\u0026quot;,\u0026quot;diagnosis\u0026quot;,\u0026quot;gender\u0026quot;), sep=\u0026quot;_\u0026quot;) mydata ## # A tibble: 405,440 x 8 ## country iso2 iso3 year new diagnosis gender value ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 Afghanistan AF AFG 1980 new sp m014 NA ## 2 Afghanistan AF AFG 1980 new sp m1524 NA ## 3 Afghanistan AF AFG 1980 new sp m2534 NA ## 4 Afghanistan AF AFG 1980 new sp m3544 NA ## 5 Afghanistan AF AFG 1980 new sp m4554 NA ## 6 Afghanistan AF AFG 1980 new sp m5564 NA ## 7 Afghanistan AF AFG 1980 new sp m65 NA ## 8 Afghanistan AF AFG 1980 new sp f014 NA ## 9 Afghanistan AF AFG 1980 new sp f1524 NA ## 10 Afghanistan AF AFG 1980 new sp f2534 NA ## # … with 405,430 more rows mydata\u0026lt;-mydata %\u0026gt;% separate(col=gender,into= c(\u0026quot;gender\u0026quot;,\u0026quot;age\u0026quot;),sep = 1) mydata ## # A tibble: 405,440 x 9 ## country iso2 iso3 year new diagnosis gender age value ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 Afghanistan AF AFG 1980 new sp m 014 NA ## 2 Afghanistan AF AFG 1980 new sp m 1524 NA ## 3 Afghanistan AF AFG 1980 new sp m 2534 NA ## 4 Afghanistan AF AFG 1980 new sp m 3544 NA ## 5 Afghanistan AF AFG 1980 new sp m 4554 NA ## 6 Afghanistan AF AFG 1980 new sp m 5564 NA ## 7 Afghanistan AF AFG 1980 new sp m 65 NA ## 8 Afghanistan AF AFG 1980 new sp f 014 NA ## 9 Afghanistan AF AFG 1980 new sp f 1524 NA ## 10 Afghanistan AF AFG 1980 new sp f 2534 NA ## # … with 405,430 more rows # change age to factor var mydata$age %\u0026gt;% unique() ## [1] \u0026quot;014\u0026quot; \u0026quot;1524\u0026quot; \u0026quot;2534\u0026quot; \u0026quot;3544\u0026quot; \u0026quot;4554\u0026quot; \u0026quot;5564\u0026quot; \u0026quot;65\u0026quot; mydata\u0026lt;-mydata %\u0026gt;% mutate( age = recode_factor( age, \u0026quot;014\u0026quot; = \u0026quot;0-14\u0026quot;, \u0026quot;1524\u0026quot; = \u0026quot;15-24\u0026quot;, \u0026quot;2534\u0026quot; = \u0026quot;25-34\u0026quot;, \u0026quot;3544\u0026quot; = \u0026quot;35-44\u0026quot;, \u0026quot;4554\u0026quot; = \u0026quot;45-54\u0026quot;, \u0026quot;5564\u0026quot; = \u0026quot;55-64\u0026quot;, \u0026quot;65\u0026quot; = \u0026quot;65 and above\u0026quot; ) ) mydata ## # A tibble: 405,440 x 9 ## country iso2 iso3 year new diagnosis gender age value ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; ## 1 Afghanistan AF AFG 1980 new sp m 0-14 NA ## 2 Afghanistan AF AFG 1980 new sp m 15-24 NA ## 3 Afghanistan AF AFG 1980 new sp m 25-34 NA ## 4 Afghanistan AF AFG 1980 new sp m 35-44 NA ## 5 Afghanistan AF AFG 1980 new sp m 45-54 NA ## 6 Afghanistan AF AFG 1980 new sp m 55-64 NA ## 7 Afghanistan AF AFG 1980 new sp m 65 and above NA ## 8 Afghanistan AF AFG 1980 new sp f 0-14 NA ## 9 Afghanistan AF AFG 1980 new sp f 15-24 NA ## 10 Afghanistan AF AFG 1980 new sp f 25-34 NA ## # … with 405,430 more rows # remove new mydata\u0026lt;-mydata %\u0026gt;% select(-new) # factor variable mydata \u0026lt;- mydata %\u0026gt;% mutate(gender = parse_factor(gender, levels = c(\u0026quot;f\u0026quot;,\u0026quot;m\u0026quot;)), diagnosis = parse_factor(diagnosis)) %\u0026gt;% rename(cases = value) mydata ## # A tibble: 405,440 x 8 ## country iso2 iso3 year diagnosis gender age cases ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; ## 1 Afghanistan AF AFG 1980 sp m 0-14 NA ## 2 Afghanistan AF AFG 1980 sp m 15-24 NA ## 3 Afghanistan AF AFG 1980 sp m 25-34 NA ## 4 Afghanistan AF AFG 1980 sp m 35-44 NA ## 5 Afghanistan AF AFG 1980 sp m 45-54 NA ## 6 Afghanistan AF AFG 1980 sp m 55-64 NA ## 7 Afghanistan AF AFG 1980 sp m 65 and above NA ## 8 Afghanistan AF AFG 1980 sp f 0-14 NA ## 9 Afghanistan AF AFG 1980 sp f 15-24 NA ## 10 Afghanistan AF AFG 1980 sp f 25-34 NA ## # … with 405,430 more rows  ","tags":["bds723"],"title":"Bds723 Tidy Data","type":"post"},{"authors":[],"categories":[],"content":"\n\n\n\n","date":1583780310,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583780310,"objectID":"5609fd056f33c752b859a8cc05b6b687","permalink":"/post/note13-kernel/","publishdate":"2020-03-09T13:58:30-05:00","relpermalink":"/post/note13-kernel/","section":"post","summary":"","tags":["bds722","bds722note","kernel"],"title":"Note13 Kernel","type":"post"},{"authors":[],"categories":[],"content":" Question 1 Suppose disease locus (alleles D and d) and LD locus (alleles A and a) with Pr(A|D) = 0.9, Pr(A|d) = 0.2 and f (D) = 0.4. Calculate allele distribution of LD locus (i.e. f (A) =? And f (a) =?) \nf(A) = f(A,D) + f(A,d) = Pr(A|D)f(D) + Pr(A|d)f(d)\nf(D) = 0.4 \\(=\u0026gt;\\) f(d) = 0.6\nPr(A|D) = 0.9, Pr(A|d) = 0.2\nf(A) = \\(0.9*0.4 + 0.2*0.6 = 0.48\\)\nf(a) = 1 - f(A) = 1 - 0.48 = 0.52\na. set Pr(A|D)=p1, Pr(A|d)=p2 and f(D)=q, write down the inference formula for LD allele frequencies (i.e. f(A) and f(a)), based on parameters of p1, p2 and q (score 15) \nf(A) = f(A,D) + f(A,d) = Pr(A|D)f(D) + Pr(A|d)f(d)\nf(D) = q \\(=\u0026gt;\\) f(d) = 1-q\nf(A) = \\(p_1*q + p_2*(1-q)\\)\nf(a) = 1 - f(A) = \\(1 - p_1*q - p_2*(1-q)\\)\nb. Extra credit: implements getAlleleFreq function using the template of lecture slide (score 5) \ngetAlleleFreq \u0026lt;- function(p1,p2,q){ fA = p1*q + p2*(1-q) fa = 1 - fA return(c(fA=fA,fa=fa)) } getAlleleFreq(0.9,0.2,0.4) ## fA fa ## 0.48 0.52  Question 2 Suppose disease locus (alleles D and d) and LD locus (alleles A and a) with Pr(A|D) = 0.9, Pr(A|d) = 0.2 and f (D) = 0.4. \na. Construct contingency table for the joint genotype distribution of disease and LD locus (score 25) \n\\(Pr(DD) = 0.4*0.4 = 0.16\\)\n\\(Pr(Dd) = 2*0.4*0.6 = 0.48\\)\n\\(Pr(dd) = 0.6*0.6 = 0.36\\)\n\\(Pr(a|D) = 1 - Pr(A|D) = 1 - 0.9 = 0.1\\)\n\\(Pr(a|d) = 1 - Pr(A|d) = 1 - 0.2 = 0.8\\)\n\\(Pr(AA,DD) = Pr(AA|DD)Pr(DD) = Pr(A|D)^2Pr(DD) = 0.9^2*0.16 = 0.1296\\)\n\\(Pr(AA,Dd) = Pr(AA,D_md_f) + Pr(AA,D_fd_m)\\)\n\\(= Pr(AA|D_md_f)Pr(D_md_f) + Pr(AA|D_fd_m)Pr(D_fd_m)\\)\n\\(= Pr(A|D_m)Pr(A|d_f)Pr(D_md_f) + Pr(A|D_f)Pr(A|d_m)Pr(D_fd_m)\\)\n\\(= Pr(A|D)Pr(A|d)Pr(D_md_f) + Pr(A|D)Pr(A|d)Pr(D_fd_m)\\)\n\\(= Pr(A|D)Pr(A|d)[Pr(D_md_f) + Pr(D_fd_m)]\\)\n\\(= Pr(A|D)Pr(A|d)Pr(Dd)\\)\n\\(= 0.9*0.2*0.48 = 0.0864\\)\n\\(Pr(AA,dd) = Pr(AA|dd)Pr(dd) = Pr(A|d)^2Pr(dd) = 0.2^2*0.36 = 0.0144\\)\n\\(Pr(Aa,DD) = Pr(A_ma_f,DD) + Pr(A_fa_m,DD)\\)\n\\(= Pr(A_ma_f|DD)Pr(DD) + Pr(A_fa_m|DD)Pr(DD)\\)\n\\(= Pr(A_m|D)Pr(a_f|D)Pr(DD) + Pr(A_f|D)Pr(a_m|D)Pr(DD)\\)\n\\(= Pr(A|D)Pr(a|D)Pr(DD) + Pr(A|D)Pr(a|D)Pr(DD)\\)\n\\(= 2Pr(A|D)Pr(a|D)Pr(DD)\\)\n\\(= 2*0.9*0.1*0.16 = 0.0288\\)\n\\(Pr(Aa,Dd) = Pr(A_ma_f,D_md_f) + Pr(A_ma_f,D_fd_m) + Pr(A_fa_m,D_md_f) + Pr(A_fa_m,D_fd_m)\\)\n\\(= Pr(A_ma_f|D_md_f)Pr(D_md_f) + Pr(A_ma_f|D_fd_m)Pr(D_fd_m) + Pr(A_fa_m|D_md_f)Pr(D_md_f) + Pr(A_fa_m|D_fd_m)Pr(D_fd_m)\\)\n\\(= Pr(A_m|D_m)Pr(a_f|d_f)Pr(D_md_f) + Pr(A_m|d_m)Pr(a_f|D_f)Pr(D_fd_m) + Pr(A_f|d_f)Pr(a_m|D_m)Pr(D_md_f) + Pr(A_f|D_f)Pr(a_m|d_m)Pr(D_fd_m)\\)\n\\(= Pr(A|D)Pr(a|d)Pr(D_md_f) + Pr(A|d)Pr(a|D)Pr(D_fd_m) + Pr(A|d)Pr(a|D)Pr(D_md_f) + Pr(A|D)Pr(a|d)Pr(D_fd_m)\\)\n\\(= Pr(A|D)Pr(a|d)[Pr(D_md_f)+Pr(D_fd_m)] + Pr(A|d)Pr(a|D)[Pr(D_fd_m)+Pr(D_md_f)]\\)\n\\(= Pr(A|D)Pr(a|d)Pr(Dd) + Pr(A|d)Pr(a|D)Pr(Dd)\\)\n\\(= 0.9*0.8*0.48+0.2*0.1*0.48 = 0.3552\\)\n\\(Pr(Aa,dd) = Pr(A_ma_f,dd) + Pr(A_fa_m,dd)\\)\n\\(= Pr(A_ma_f|dd)Pr(dd) + Pr(A_fa_m|dd)Pr(dd)\\)\n\\(= Pr(A_m|d)Pr(a_f|d)Pr(dd) + Pr(A_f|d)Pr(a_m|d)Pr(dd)\\)\n\\(= Pr(A|d)Pr(a|d)Pr(dd) + Pr(A|d)Pr(a|d)Pr(dd)\\)\n\\(= 2Pr(A|d)Pr(a|d)Pr(dd)\\)\n\\(= 2*0.2*0.8*0.36 = 0.1152\\)\n\\(Pr(aa,DD) = Pr(aa|DD)Pr(DD) = Pr(a|D)^2Pr(DD)\\)\n\\(= 0.1^2*0.16 = 0.0016\\)\n\\(Pr(aa,Dd) = Pr(aa,D_md_f) + Pr(aa,D_fd_m)\\)\n\\(= Pr(aa|D_md_f)Pr(D_md_f) + Pr(aa|D_fd_m)Pr(D_fd_m)\\)\n\\(= Pr(a|D_m)Pr(a|d_f)Pr(D_md_f) + Pr(a|D_f)Pr(a|d_m)Pr(D_fd_m)\\)\n\\(= Pr(a|D)Pr(a|d)Pr(D_md_f) + Pr(a|D)Pr(a|d)Pr(D_fd_m)\\)\n\\(= Pr(a|D)Pr(a|d)[Pr(D_md_f)+Pr(D_fd_m)]\\)\n\\(= Pr(a|D)Pr(a|d)Pr(Dd)\\)\n\\(= 0.1*0.8*0.48 = 0.0384\\)\n\\(Pr(aa,dd) = Pr(aa|dd)Pr(dd) = Pr(a|d)^2Pr(dd)\\)\n\\(= 0.8^2*0.36 = 0.2304\\)\n  ``` AA Aa aa    DD 0.1296 0.0288 0.0016  Dd 0.0864 0.3552 0.0384  dd 0.0144 0.1152 0.2304    b. Calculate genotype distribution of the LD locus (i.e. f(AA), f(Aa) and f(aa)), based on the contingency table (score 10) \n\\(Pr(AA) = Pr(AA,DD) + Pr(AA,Dd) + Pr(AA,dd)\\)\n\\(= 0.1296 + 0.0864 + 0.0144 = 0.2304\\)\n\\(Pr(Aa) = Pr(Aa,DD) + Pr(Aa,Dd) + Pr(Aa,dd)\\)\n\\(= 0.0288 + 0.3552 + 0.1152 = 0.4992\\)\n\\(Pr(aa) = Pr(aa,DD) + Pr(aa,Dd) + Pr(aa,dd)\\)\n\\(= 0.0016 + 0.0384 + 0.2304 = 0.2704\\)\nc. Is LD locus in HWE? (score 10)\nYes. Because \\(Pr(AA) = Pr(A)^2, Pr(Aa) = 2Pr(A)Pr(a), Pr(aa) = Pr(a)^2\\)\nd. set Pr(A|D)=p1, Pr(A|d)=p2 and f(D)=q, write down the inference formula for LD genotype frequencies (i.e. f(AA), f(Aa) and f(aa)), based on parameters of p1, p2 and q. (score 10) \n\\(Pr(AA) = Pr(AA,DD) + Pr(AA,Dd) + Pr(AA,dd)\\)\n\\(= Pr(AA|DD)Pr(DD) + Pr(AA|Dd)Pr(Dd) + Pr(AA|dd)Pr(dd)\\)\n\\(= Pr(A|D)^2Pr(DD) + Pr(A|D)Pr(A|d)Pr(Dd) + Pr(A|d)^2Pr(dd)\\)\n\\(= p_1^2q^2 + p_1p_22q(1-q) + p_2^2(1-q)^2\\)\n\\(Pr(Aa) = Pr(Aa,DD) + Pr(Aa,Dd) + Pr(Aa,dd)\\)\n\\(= Pr(Aa|DD)Pr(DD) + Pr(Aa|Dd)Pr(Dd) + Pr(Aa|dd)Pr(dd)\\)\n\\(= 2Pr(A|D)Pr(a|D)Pr(DD) + Pr(Dd)[Pr(A|D)Pr(a|d) + Pr(A|d)Pr(a|D] + 2Pr(A|d)Pr(a|d)Pr(dd)\\)\n\\(= 2p_1(1-p_1)q^2 + 2q(1-q)[p_1(1-p_2)+p_2(1-p_1)] + 2p_2(1-p_2)(1-q)^2\\)\n\\(Pr(aa) = Pr(aa,DD) + Pr(aa,Dd) + Pr(aa,dd)\\)\n\\(= Pr(aa|DD)Pr(DD) + Pr(aa|Dd)Pr(Dd) + Pr(aa|dd)Pr(dd)\\)\n\\(= Pr(a|D)^2Pr(DD) + Pr(a|D)Pr(a|d)Pr(Dd) + Pr(a|d)^2Pr(dd)\\)\n\\(= (1-p_1)^2q^2 + (1-p_1)(1-p_2)2q(1-q) + (1-p_2)^2(1-q)^2\\)\ne. Extra credit: implements getGenotypeFreq function using the template of lecture slide (score 10) \ngetGenotypeFreq \u0026lt;- function(p1,p2,q){ fAA = (p1^2)*(q^2) + p1*p2*2*q*(1-q) + (p2^2)*((1-q)^2) fAa = 2*p1*(1-p1)*(q^2) + 2*q*(1-q)*(p1*(1-p2)+p2*(1-p1)) + 2*p2*(1-p2)*((1-q)^2) faa = ((1-p1)^2)*(q^2) + (1-p1)*(1-p2)*2*q*(1-q) + ((1-p2)^2)*((1-q)^2) return(c(fAA=fAA,fAa=fAa, faa = faa)) } getGenotypeFreq(0.9,0.2,0.4) ## fAA fAa faa ## 0.2304 0.4992 0.2704  Question 3 Suppose disease locus (alleles D and d) and LD locus (alleles A and a) with Pr(A|D) = 0.9, Pr(A|d) = 0.2 and f (D) = 0.4. If penetrance of disease locus G1 is: Pr(Y = 1|G1 = DD) = 0.8, Pr(Y = 1|G1 = Dd) = 0.4, and Pr(Y = 1|G1 = dd) = 0.2, what the penetrance of LD locus G2 will be? (score 30) \n\\(Pr(Y|G_2=AA) = Pr(Y|G_2=Aa) = Pr(Y|G_2=aa) = Pr(Y)\\) Due to \\(G_2\\) is independent with Y.\n\\(Pr(DD) = Pr(D)^2 = 0.4^2 = 0.16\\)\n\\(Pr(Dd) = 2Pr(D)Pr(d) = 2*0.4*0.6 = 0.48\\)\n\\(Pr(dd) = Pr(d)^2 = 0.6^2 = 0.36\\)\n\\(Pr(Y=1) = Pr(Y=1, G_1=DD)+Pr(Y=1, G_1=Dd)+Pr(Y=1, G_1=dd)\\)\n\\(= Pr(Y=1|DD)Pr(DD)+Pr(Y=1|Dd)Pr(Dd)+Pr(Y=1|dd)Pr(dd)\\)\n\\(= 0.8*0.16 + 0.4*0.48 + 0.2*0.36 = 0.392\\)\n\\(Pr(Y=0) = Pr(Y=0, G_1=DD)+Pr(Y=0, G_1=Dd)+Pr(Y=0, G_1=dd)\\)\n\\(= Pr(Y=0|DD)Pr(DD)+Pr(Y=0|Dd)Pr(Dd)+Pr(Y=0|dd)Pr(dd)\\)\n\\(= [1-Pr(Y=1|DD)]Pr(DD)+[1-Pr(Y=1|Dd)]Pr(Dd)+[1-Pr(Y=1|dd)]Pr(dd)\\)\n\\(= (1-0.8)*0.16 + (1-0.4)*0.48 + (1-0.2)*0.36 = 0.608\\)\n ","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583444923,"objectID":"e62721fff5fd6f4f1bbe226b75c205a8","permalink":"/post/bds751hw7/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/bds751hw7/","section":"post","summary":"Question 1 Suppose disease locus (alleles D and d) and LD locus (alleles A and a) with Pr(A|D) = 0.9, Pr(A|d) = 0.2 and f (D) = 0.4. Calculate allele distribution of LD locus (i.e. f (A) =? And f (a) =?) \nf(A) = f(A,D) + f(A,d) = Pr(A|D)f(D) + Pr(A|d)f(d)\nf(D) = 0.4 \\(=\u0026gt;\\) f(d) = 0.6\nPr(A|D) = 0.9, Pr(A|d) = 0.2\nf(A) = \\(0.9*0.4 + 0.","tags":["bds751","bds751hw"],"title":"Homework 7","type":"post"},{"authors":[],"categories":[],"content":" Possion Regression Model Objective Possion regression is the most common technique applied to model count data.\nPossion Distribution\nPoisson distribution describes the number of events that occur in a given time period where its mean \\(\\lambda\\) is the average number of events per period.\n\\(Pr(Y=y) = (\\frac{e^{-\\lambda}\\lambda^y}{y!})\\)\n E(y) = Var(y) = \\(\\lambda\\)   Model Structure \\(ln(E(Y)) = ln(\\lambda) = X\\beta = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_3 + ...\\)\n link function: log   Model Assumption  \\(y_i\\) are independent of one another, and each \\(y_i\\) is a non-negative integer.\n Each \\(y_i\\) follows the Poisson distribution with mean \\(\\lambda_i\\)\n The ln(E(Y)) is a linear function of x.\n The dependent variable is not over-dispersed and does not have an excessive number of zeros.\n   Parameter estimates and interpretation  Model Fit   Negative Binomial Regression Model Objective  Negative Binomial Regression can be used to model count data with over-dispersion.\n Over-dispersion means that there is more variation in the response than the model expected.\n   Model Structure \\(ln(E(Y)) = ln(\\lambda) = X\\beta = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_3 + ...\\)\n link function: log  Possion Distribution\n\\(Pr(Y=y | \\lambda, \\alpha) = \\frac{\\Gamma(y+\\alpha^{-1})}{y!\\Gamma(\\alpha^{-1})}(\\frac{\\alpha^{-1}}{\\alpha^{-1}+\\lambda})^{\\alpha-1}(\\frac{\\lambda}{\\alpha^{-1}+\\lambda})\\)\n The negative binomial distribution has two parameters: λ and α\n λ is the mean or expected value of the distribution\n α is the over dispersion parameter\n When α = 0 the negative binomial distribution is the same as a Poisson distributio\n   Model Assumption  The response variable is a count variable and each subject has the same length of observation time.\n The dependent variable is over-dispersed and does not have an excessive number of zeros.\n    ","date":1582952955,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582952955,"objectID":"39c0995200b94f420e70fd3e8e54f720","permalink":"/post/note2-regression-for-count-data/","publishdate":"2020-02-28T23:09:15-06:00","relpermalink":"/post/note2-regression-for-count-data/","section":"post","summary":"Possion Regression Model Objective Possion regression is the most common technique applied to model count data.\nPossion Distribution\nPoisson distribution describes the number of events that occur in a given time period where its mean \\(\\lambda\\) is the average number of events per period.\n\\(Pr(Y=y) = (\\frac{e^{-\\lambda}\\lambda^y}{y!})\\)\n E(y) = Var(y) = \\(\\lambda\\)   Model Structure \\(ln(E(Y)) = ln(\\lambda) = X\\beta = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_3 + .","tags":["bds722","bds722","poisson"],"title":"Note2 Regression for Count Data","type":"post"},{"authors":[],"categories":[],"content":" Question 1 Following is a sample of 1,000 gametes produced by a trihybrid individual. Please answer the following questions. Label each gamete (or haplotype) as non-recombinant, single recombinant or double recombinant. (Score 15)\nRecombinant probability \\(\\theta\\) has to be \\(\\leq 0.5\\) and non-recombinat probability has to be \\(\\geq 0.5\\). There are 1000 gametes and the total number of the two non-recombinant haplotypes shoule be greater or equal than 500, hence Btr|bTR should be the individual’s diplotype. Below is my infer,\n  Haplotype Label    BTR single recombinant  btr single recombinant  Btr non-recombinant  bTR non-recombinant  BtR single recombinant  bTr single recombinant  BTr double recombinant  btR double recombinant     Question 2 What is the probability of following pedigree genotype? If f(D)=0.1 and genotypes for subject 1, 2, 3, 4, and 5 are G1 = DD, G2 = DD, G3 = G4 = G5 = Dd? (Score 15)\n\\(Pr(D) = 0.1 =\u0026gt; Pr(d) = 0.9\\)\n\\(Pr(DD) = 0.01, Pr(Dd) = 0.18, Pr(dd) = 0.81\\)\n\\(Pr(G1, G2, G3, G4, G5)\\)\n\\(= Pr(G1)Pr(G2)Pr(G3 | G1, G2)Pr(G4)Pr(G5|G3, G4)\\)\nBecause there is no way \\(G3\\) can be Dd given her parents don’t contain d in there genotype, \\(Pr(G3 | G1, G2) = 0\\).\nHence,\n\\(Pr(G1, G2, G3, G4, G5) = 0\\)\n Question 3 Suppose an autosome locus has alleles of D and d with f(D)=0.1 and penetrance, Pr(Y = 1|G1 = DD) = 0.8, Pr(Y = 1|G1 = Dd) = 0.4, and Pr(Y = 1|G1 = dd) = 0.2. We collected a sample as following and all individuals have genotypes of Dd. What is the sample’s joint probability Pr(Y1, Y2, Y3, Y4, Y5, G1, G2, G3, G4, G5)? (Score 15)\n\\(Pr(D) = 0.1 =\u0026gt; Pr(d) = 0.9\\)\n\\(Pr(DD) = 0.01, Pr(Dd) = 0.18, Pr(dd) = 0.81\\)\n\\(Pr(Y=1 | G1 = DD = 0.8) =\u0026gt; Pr(Y=0 | G1 = DD = 0.2)\\)\n\\(Pr(Y=1 | G1 = Dd = 0.4) =\u0026gt; Pr(Y=0 | G1 = Dd = 0.6)\\)\n\\(Pr(Y=1 | G1 = dd = 0.2) =\u0026gt; Pr(Y=0 | G1 = DD = 0.8)\\)\n\\(Pr(Y1, Y2, Y3, Y4, Y5, G1, G2, G3, G4, G5)\\)\n\\(= Pr(Y1|G1)*Pr(Y2|G2)*Pr(Y3|G3)*Pr(Y4|G4)*Pr(Y5|G5)*Pr(G1,G2,G3,G4,G5)\\)\nall individuals are independent\n\\(= Pr(Y1|G1)*Pr(Y2|G2)*Pr(Y3|G3)*Pr(Y4|G4)*Pr(Y5|G5)*Pr(G1)*Pr(G2)*Pr(G3)*Pr(G4)*Pr(G5)\\)\n\\(= Pr(Y=0|G1=Dd)^2 * Pr(Y=1|G1 = Dd)^3 * Pr(Dd)^5\\)\n\\(= 0.6^2 * 0.4^2 * 0.18^5 = 0.00001088391\\)\n Question 4 Suppose an autosome locus has alleles of D and d with f(D)=0.1 and penetrance, Pr(Y = 1|G1 = DD) = 0.8, Pr(Y = 1|G1 = Dd) = 0.4, and Pr(Y = 1|G1 = dd) = 0.2. We collected a sample as following and we do not have their genotypes. What is the sample’s joint probability Pr(Y1, Y2, Y3, Y4, Y5, G1, G2, G3, G4, G5)? (Score 15)\n\\(Pr(D) = 0.1 =\u0026gt; Pr(d) = 0.9\\)\n\\(Pr(DD) = 0.01, Pr(Dd) = 0.18, Pr(dd) = 0.81\\)\n\\(Pr(Y=1 | G1 = DD = 0.8) =\u0026gt; Pr(Y=0 | G1 = DD = 0.2)\\)\n\\(Pr(Y=1 | G1 = Dd = 0.4) =\u0026gt; Pr(Y=0 | G1 = Dd = 0.6)\\)\n\\(Pr(Y=1 | G1 = dd = 0.2) =\u0026gt; Pr(Y=0 | G1 = DD = 0.8)\\)\n\\(Pr(Y=0)\\)\n\\(= Pr(Y=0, DD) + Pr(Y=0, Dd) + Pr(Y=0, dd)\\)\n\\(= Pr(Y=0|DD)Pr(DD) + Pr(Y=0|Dd)Pr(Dd) + Pr(Y=0|dd)Pr(dd)\\)\n\\(= 0.2*0.01 + 0.6*0.18 + 0.8*0.81 = 0.758\\)\n\\(Pr(Y=1)\\)\n\\(= Pr(Y=1, DD) + Pr(Y=1, Dd) + Pr(Y=1, dd)\\)\n\\(= Pr(Y=1|DD)Pr(DD) + Pr(Y=1|Dd)Pr(Dd) + Pr(Y=1|dd)Pr(dd)\\)\n\\(= 0.8*0.01 + 0.4*0.18 + 0.2*0.81 = 0.242\\)\nWe don’t know the sample’s genotypes and all of them are independent, so, when calculating the joint probability we can just ignore their genotypes.\n\\(Pr(Y1, Y2, Y3, Y4, Y5, G1, G2, G3, G4, G5)\\)\n\\(= Pr(Y1)*Pr(Y2)*Pr(Y3)*Pr(Y4)*Pr(Y5)\\)\n\\(= Pr(Y=0)^2*Pr(Y=1)^3\\)\n\\(= 0.758^2 * 0.242^3 = 0.00814300139\\)\n Question 5 Suppose an autosome locus has alleles of D and d with f(D)=0.1 and penetrance, Pr(Y = 1|G1 = DD) = 0.8, Pr(Y = 1|G1 = Dd) = 0.4, and Pr(Y = 1|G1 = dd) = 0.2. We collected a sample as following and all individuals have genotypes of Dd. What is the sample’s joint probability Pr(Y1, Y2, Y3, Y4, Y5, G1, G2, G3, G4, G5)? (Score 20)\n\\(Pr(D) = 0.1 =\u0026gt; Pr(d) = 0.9\\)\n\\(Pr(DD) = 0.01, Pr(Dd) = 0.18, Pr(dd) = 0.81\\)\n\\(Pr(Y=1 | G1 = DD = 0.8) =\u0026gt; Pr(Y=0 | G1 = DD = 0.2)\\)\n\\(Pr(Y=1 | G1 = Dd = 0.4) =\u0026gt; Pr(Y=0 | G1 = Dd = 0.6)\\)\n\\(Pr(Y=1 | G1 = dd = 0.2) =\u0026gt;Pr(Y=0 | G1 = DD = 0.8)\\)\n\\(Pr(Y1, Y2, Y3, Y4, Y5, G1, G2, G3, G4, G5)\\)\n\\(= Pr(Y1|G1)*Pr(Y2|G2)*Pr(Y3|G3)*Pr(Y4|G4)*Pr(Y5|G5)*Pr(G1,G2,G3,G4,G5)\\)\n\\(= Pr(Y1|G1)*Pr(Y2|G2)*Pr(Y3|G3)*Pr(Y4|G4)*Pr(Y5|G5)*Pr(G1)*Pr(G2)*Pr(G3|G1,G2)*Pr(G4)*Pr(G5)\\)\n\\(= Pr(Y=0|G1=Dd)^2*Pr(Y=1|G1=Dd)^3*Pr(Dd)^4*0.5\\)\n\\(= 0.6^2 * 0.4^3 * 0.18^4 * 0.5 = 0.00001209323\\)\n Question 6 Suppose an autosome locus has alleles of D and d with f(D)=0.1 and penetrances, Pr(Y = 1|G1 = DD) = 0, Pr(Y = 1|G1 = Dd) = 0, and Pr(Y = 1|G1 = dd) = 1. We collected a sample as following and we do not have their genotypes. What is the sample’s joint probability Pr(Y1, Y2, Y3, Y4, Y5, G1, G2, G3, G4, G5)? (score 20)\n\\(Pr(D) = 0.1 =\u0026gt; Pr(d) = 0.9\\)\n\\(Pr(DD) = 0.01, Pr(Dd) = 0.18, Pr(dd) = 0.81\\)\n\\(Pr(Y=1 | G1 = DD = 0) =\u0026gt; Pr(Y=0 | G1 = DD = 1)\\)\n\\(Pr(Y=1 | G1 = Dd = 0) =\u0026gt; Pr(Y=0 | G1 = Dd = 1)\\)\n\\(Pr(Y=1 | G1 = dd = 1) =\u0026gt;Pr(Y=0 | G1 = DD = 0)\\)\nBased on the penetrances, we can conclude that people who has been affected have only one possiable genotype: dd. So, individual 2, individual 3 and individual 5 have genotype dd.\nBecause invidual 3’s genotype(dd) is inherited from individual 2(dd) and individual 1(?). And for people who are not affected there are two possiable genotypes: DD and Dd, and individual 1’s genotype can only be Dd.\nSo till now, there is only one individual’s genotype is unknown - individual 4. Due to 4 has not been affected, we will consider two genotypes for him(DD and Dd) when calculating the joint probability.\n\\(Pr(Y1, Y2, Y3, Y4, Y5, G1, G2, G3, G4, G5)\\)\n\\(= Pr(Y1|G1)*Pr(Y2|G2)*Pr(Y3|G3)*Pr(Y4|G4)*Pr(Y5|G5)*Pr(G1,G2,G3,G4,G5)\\)\n\\(= Pr(Y1|G1)*Pr(Y2|G2)*Pr(Y3|G3)*Pr(Y4|G4)*Pr(Y5|G5)*Pr(G1)*Pr(G2)*Pr(G3|G1,G2)*Pr(G4)*Pr(G5)\\)\n\\(Pr(Y1|G1) = Pr(Y=0|Dd) = 1\\)\n\\(Pr(Y2|G2) = Pr(Y=1|dd) = 1\\)\n\\(Pr(Y3|G3) = Pr(Y=1|dd) = 1\\)\n\\(Pr(Y4|G4) = Pr(Y=0|DDorDd) = 1\\)\n\\(Pr(Y5|G5) = Pr(Y=1|dd) = 1\\)\n\\(Pr(G1) = Pr(Dd) = 0.18\\)\n\\(Pr(G2) = Pr(G3) = Pr(G5) = Pr(dd) = 0.81\\)\n\\(Pr(G3|G1,G2) = Pr(dd|Dd,dd) = 0.5\\)\nAns = \\(1*1*1*1*1*0.18*0.81^3*0.5 = 0.04782969\\)\n ","date":1582862371,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582862371,"objectID":"d0bd4c838b573bf8576b4ddceb02d0f0","permalink":"/post/bds751hw6/","publishdate":"2020-02-27T21:59:31-06:00","relpermalink":"/post/bds751hw6/","section":"post","summary":"Question 1 Following is a sample of 1,000 gametes produced by a trihybrid individual. Please answer the following questions. Label each gamete (or haplotype) as non-recombinant, single recombinant or double recombinant. (Score 15)\nRecombinant probability \\(\\theta\\) has to be \\(\\leq 0.5\\) and non-recombinat probability has to be \\(\\geq 0.5\\). There are 1000 gametes and the total number of the two non-recombinant haplotypes shoule be greater or equal than 500, hence Btr|bTR should be the individual’s diplotype.","tags":["bds751","bds751hw"],"title":"Homework 6","type":"post"},{"authors":[],"categories":[],"content":" Assessing the Link Function  Pregibon link test\n Run GLM.\n Obtain fitted values for linear predictor.\n Create new variables \\((X\\hat{\\beta})^2\\).\n Run model with \\(X\\hat{\\beta} + (X\\hat{\\beta})^2\\) as predictors.\n evaluate: if p-value is not significant for \\((X\\hat{\\beta})^2\\) term, link function is correctly specified.\n  The p-value for _hatsq is greater 0.05 which is not statistically significant indicatiing that the model seems to be well specified.\n Outlier Detection  Cook’s distance measures the aggregate change in the estimated coefficients when each observation is left out of the estimation.\n Rules of thumb:  \\(C_i \u0026gt; \\frac{4}{n-p-1}\\) n = number of total observations. p = number of coefficients\n \\(C_i \u0026gt; \\frac{3\\sum{C_i}}{n}\\)\n \\(C_i \u0026gt; 1\\)\n    R-square In ordinary least squares(OLS) regression, \\(R^2 = 1 - \\frac{\\sum_{i=1}^N(y_i - \\hat{y_i})^2}{\\sum_{i=1}^N(y_i - \\bar{y})^2}\\)\n N is the number of pbservations in the model.\n y is the dependent variable.\n y-hat is the value predicted by the model.\n  \\(R^2\\) is typically interpreted as % variance explained, but \\(Pseudo-R^2 = (correlation(y,\\hat{y}))^2\\)\n Information Criteria The AIC and BIC are two popular measures for comparing maximum likelihood models.1\n AIC = Akaike Information Criterion\n \\(AIC = -2*ln(likelihood) + 2*k\\)\n BIC = Bayesian Information Criterion\n \\(BIC = -2*ln(likelihood) + ln(N)*k\\)\n k = number of parameters estimated\n N = number of observations\n  AIC and BIC can be viewed as measures that combine fit and complexity.\nFit is measured negatively by −2 × ln(likelihood); the larger the value, the worse the fit.\nComplexity is measured positively, either by 2 × k (AIC) or ln(N) × k (BIC).\nGiven two models fit on the same data, the model with the smaller value of the information criterion is considered to be better.\n Residuals   https://www.stata.com/manuals/rbicnote.pdf↩\n   ","date":1582840596,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582840596,"objectID":"87c84920b9c36941a024c5f086843e15","permalink":"/post/note8-glm-diagnostics/","publishdate":"2020-02-27T15:56:36-06:00","relpermalink":"/post/note8-glm-diagnostics/","section":"post","summary":"Assessing the Link Function  Pregibon link test\n Run GLM.\n Obtain fitted values for linear predictor.\n Create new variables \\((X\\hat{\\beta})^2\\).\n Run model with \\(X\\hat{\\beta} + (X\\hat{\\beta})^2\\) as predictors.\n evaluate: if p-value is not significant for \\((X\\hat{\\beta})^2\\) term, link function is correctly specified.\n  The p-value for _hatsq is greater 0.05 which is not statistically significant indicatiing that the model seems to be well specified.","tags":["bds722","bds722note","glm"],"title":"Note8 Glm Diagnostics","type":"post"},{"authors":[],"categories":[],"content":" Question 1 Suppose the genetic distance is d Morgan between two loci (i.e. average d crossovers will occur per meiosis). Infer recombination probability θ between the two loci per meiosis (i.e. the map function, θ(d)) (Score 20)\nx: number of observed crossovers\nd: genetic distance between two loci θ(d): prob of recombination between two loci per meiosis\n\\(f(X = x) = \\frac{e^{-d}d^x}{x!},x = 0,1,2,..\\)\n\\(\\theta(d) = \\sum_{odd x}f(X=x)\\)\n\\(= \\sum_{oddx}\\frac{e^{-d}d^x}{x!}\\)\n\\(= \\frac{1}{2}(\\sum_{oddx}\\frac{e^{-d}d^x}{x!} + \\sum_{oddx}\\frac{e^{-d}d^x}{x!} + \\sum_{evenx}\\frac{e^{-d}d^x}{x!} - \\sum_{evenx}\\frac{e^{-d}d^x}{x!})\\)\n\\(= \\frac{e^{-d}}{2}(\\sum_{oddx}\\frac{d^x}{x!}+(\\sum_{oddx}\\frac{d^x}{x!}+\\sum_{evenx}\\frac{d^x}{x!})-\\sum_{evenx}\\frac{d^x}{x!})\\)\n\\(= \\frac{e^{-d}}{2}(-\\sum_{oddx}\\frac{(-d)^x}{x!} + \\sum_{x=0}^\\infty\\frac{d^x}{x!} - \\sum_{evenx}\\frac{(-d)^x}{x!})\\)\n\\(= \\frac{e^{-d}}{2}(\\sum_{x=0}^\\infty\\frac{d^x}{x!}-(\\sum_{oddx}\\frac{(-d)^x}{x!} + \\sum_{evenx}\\frac{(-d)^x}{x!}))\\)\n\\(= \\frac{e^{-d}}{2}(\\sum_{x=0}^\\infty\\frac{d^x}{x!} - \\sum_{x=0}^\\infty\\frac{(-d)^x}{x!})\\)\nBased on Taylor Series\n\\(e^d = \\sum_{x=0}^\\infty\\frac{d^x}{x!}, x = 0,1,2,...\\)\n\\(e^{-d} = \\sum_{x=0}^\\infty\\frac{(-d)^x}{x!}\\)\nHence,\n\\(\\theta(d) = \\frac{e^{-d}}{2}(e^d - e^{-d}) = \\frac{1-e^{-2d}}{2}\\)\n Question 2 Suppose the recombinatin probability between two loci is θ. (Score 20)\na. If parent genotype for the two loci is dm|Dm, what are transmitted haplotypes and their probabilities?\ntransmitted haplotypes: dm, Dm\nPr(dm) = Pr(Dm) = 0.5\nb. If parent genotype for the two loci is Dm|DM, what are transmitted haplotypes and their probabilities?\ntransmitted haplotypes: Dm, DM\nPr(Dm) = Pr(DM) = 0.5\nc. If parent genotype for the two loci is dm|DM, what are transmitted haplotypes and their probabilities?\ntransmitted haplotypes: dm, DM, dM, Dm\nPr(dm) = Pr(DM) = \\(\\frac{1-\\theta}{2}\\)\nPr(dM) = Pr(Dm) = \\(\\frac{\\theta}{2}\\)\nd. If parent genotype for the two loci is dM|Dm, what are transmitted haplotypes and their probabilities?\ntransmitted haplotypes: dM, Dm, dm, DM\nPr(dM) = Pr(Dm) = \\(\\frac{1-\\theta}{2}\\)\nPr(dm) = Pr(DM) = \\(\\frac{\\theta}{2}\\)\n Question 3 Suppose a family pedigree is as follows. (Score 35)\na. What are recombinant haplotypes?\nDm and dM\nb. What are non-recombinant haplotypes?\nDM, dm\nc. Estimate recombination probability θ and what is the standard error?\n\\(n = 5, r = 1, s = 4\\)\n\\(\\theta = \\frac{1}{5} = 0.2\\)\n\\(se = \\sqrt{\\frac{rs}{n^3}} = \\sqrt{\\frac{4}{125}}=0.179\\)\nd. We want to test if there is independent assortment between the two loci. What is the \\(H_0\\) and \\(H_1\\)?\n\\(H_0\\) : \\(\\theta = 0.5\\)\n\\(H_1\\) : \\(\\theta \\leq 0.5\\)\ne. Conduct a statistical test of question d, and show your conclusion. (Hint: binomial test)\nbinom.test(x=1,n=5,p=0.5,alternative=c(\u0026quot;less\u0026quot;),conf.level=0.95) ## ## Exact binomial test ## ## data: 1 and 5 ## number of successes = 1, number of trials = 5, p-value = 0.1875 ## alternative hypothesis: true probability of success is less than 0.5 ## 95 percent confidence interval: ## 0.0000000 0.6574083 ## sample estimates: ## probability of success ## 0.2 The p-value for our test is 0.1875 which is greater than 0.05, so we failed to reject the null hypotheis and conclude that \\(\\theta = 0.5\\) whcih means there is independent assortment between the two loci.\nf. Extra Credit (Score 5): Suppose we do not know father’s genotype in the pedigree. Can we infer the father’s genotype with haplotype information and what are they?\nYes, we can. Offspring’s genotypes contain allele D/d and allele M/m, so, father’s genotype must contain D,d,M,m.\nAnd among offspring, there are three types of haplotypes came from father, they are DM, dm and Dm.\nHence, father’s genotype should be informative, and there are two possiable genotypes for father.\n1. DM|dm 2. Dm|dM\n Question 4 Weather report shows that snowing probability on Friday is 60%. Big data analysis indicates that the probability of UMMC cancelling class is 70% if there is snow, and the probability is 5% if there is no snow. (Score: 15)\na. Construct the contingency table of probability (Score: 5)\n\\(Pr(snow) = 0.6\\)\n\\(Pr(no-snow) = 0.4\\)\n\\(Pr(cancel | snow) = 0.7\\)\n\\(Pr(cancel | no-snow) = 0.05\\)\n\\(Pr(snow, cancel) = Pr(cancel | snow)Pr(snow) = 0.7*0.6 = 0.42\\)\n\\(Pr(snow, no-cancel) = Pr(snow) - Pr(snow, cancel) = 0.6 - 0.42 = 0.18\\)\n\\(Pr(no-snow, cancel) = Pr(cancel | no-snow)Pr(no-snow) = 0.05*0.4 = 0.02\\)\n\\(Pr(no-snow, no-cancel) = Pr(no-snow) - Pr(no-snow, cancel) = 0.4 - 0.02 = 0.38\\)\n  ~~~~~ cancel no-cancel ~~~~~    snow 0.42 0.18 0.6  no-snow 0.02 0.38 0.4  ~~~~~ 0.44 0.56 1    b. What is the probability that class will be cancelled on Friday? (Score: 5)\nPr(cancel) = Pr(cancel, snow) + Pr(cancel, no-snow) = 0.42 + 0.02 = 0.44\nc. What is the probability that there is snow on Friday if class is canceled? (Score: 5)\n\\(Pr(snow | cancel) = \\frac{Pr(snow,cancel)}{Pr(cancel)} = \\frac{0.42}{0.44} = 0.955\\)\n Question 5 A prospective study collected 100 smokers and 100 non-smokers. After 10 years, the study observed 30 smokers and 10 non-smokers got lung cancer. If our objective is that smoking is associated with cancer risk, then\na. Write down the hypothesis and estimate the parameters for hypothesis test (Score: 10) \n\\(H_0\\) : Pr(cancer, smoker) = Pr(cancer, non-smoker)\n\\(H_a\\) : Pr(cancer, smoker) \\(\\neq\\) Pr(cancer, non-smoker)\n\\(Pr(cancer,smoker) = \\frac{30}{100} = 0.3\\)\n\\(Pr(cancer,non-smoker) = \\frac{10}{100} = 0.1\\)\nb. perform a statistical test, get the statistic with p-value and show your conclusion (Score: 10) \n  ~~~~~ cancer no-cancer ~~~~~    smoker 30 70 100  non-smoker 10 90 100  ~~~~~ 40 160 200    \\(\\hat{p_1} = 0.3, x_1 = 30, n_1 = 100\\)\n\\(\\hat{p_2} = 0.1, x_2 = 10, n_2 = 100\\)\n\\(\\bar{p} = \\frac{x_1 + x_2}{n_1 + n_2} = \\frac{40}{200} = 0.2\\)\n\\(\\hat{\\sigma}_{\\hat{p_1} - \\hat{p_2}} = \\sqrt{\\frac{\\bar{p}(1-\\bar{p})}{n_1} + \\frac{\\bar{p}(1-\\bar{p})}{n_2}}\\)\n$ = = 0.0566$\n\\(z = \\frac{\\hat{p_1} - \\hat{p_2}}{\\hat{\\sigma}_{\\hat{p_1} - \\hat{p_2}}} = \\frac{0.3-0.1}{0.0566} = 3.54\\)\nThe P-Value is .0004.\nBecause our p-value is 0.0004 which is less than 0 at 0.05 alpha level, we reject the null hypothesis and conclude that smoking is associated with cancer risk.\n ","date":1582480544,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582480544,"objectID":"7904998e8c5cb9a97bf058c457f17f8e","permalink":"/post/bds751hw5/","publishdate":"2020-02-23T11:55:44-06:00","relpermalink":"/post/bds751hw5/","section":"post","summary":"Question 1 Suppose the genetic distance is d Morgan between two loci (i.e. average d crossovers will occur per meiosis). Infer recombination probability θ between the two loci per meiosis (i.e. the map function, θ(d)) (Score 20)\nx: number of observed crossovers\nd: genetic distance between two loci θ(d): prob of recombination between two loci per meiosis\n\\(f(X = x) = \\frac{e^{-d}d^x}{x!},x = 0,1,2,..\\)\n\\(\\theta(d) = \\sum_{odd x}f(X=x)\\)\n\\(= \\sum_{oddx}\\frac{e^{-d}d^x}{x!","tags":["bds751","bds751hw"],"title":"Homework 5","type":"post"},{"authors":[],"categories":[],"content":" Testing the Proportional Hazard Assumptions Schoenfeld Residuals1  Instead of a single residual for each individual, there is a separate residual for each individual for each covariate.\n Schonefeld residuals are not defined for censored individuals.\n  \\(Schonefeld residual = x_{ik} - \\sum_{i=1}^{j\\in R(t_i)} x_{kj}P_j\\) - The Schoenfeld residual is the covariate-value, \\(x_{ik}\\), for the person i who actually died at time \\(t_i\\) minus the expected value of the covariate for the risk set at \\(t_i\\).\n Plot Schoenfeld residuals against time to evaluate PH assumption.\n If the slope is not zero then the proportional hazard assumption has been violated.\n   Kaplan-Meier Curves2  If the predictor satisfy the proportional hazard assumption then the graph of the survival function versus the survival time should results in a graph with parallel curves.\n This method does not work well for continuous predictor or categorical predictors that have many levels because the graph becomes to “cluttered”.\n   Log-log Plot3  The log(-log(survival)) versus log of survival time graph should result in parallel lines if the predictor is proportional.    General Model Fit  Theorem: Suppose T is a continuous nonnegative random variable with cumulative hazard function \\(\\Lambda\\). Then the random variaable $ Y = (T)$ follows an exponential distribution with rate\\(\\lambda = 1\\).\n One way of checking the validty of a model is by comparing the model’s estimates {\\(\\hat{\\Lambda}(t_i)\\)} against the standard exponential distribution.\n  Cox-Snell Residuals \\(\\hat{e}_i = \\hat{\\Lambda_0}(t_i) exp(x_i^T \\hat{\\beta})\\)\n Application: to examine the overall fit of a Cox model: by plotting the Cox-Snell residual against the cumulative hazard function.\n A well fitting model will exhibit a linear line through the origin with a unit gradien.\n These residuals should behave like a sample from an exponential distribution with a mean equal to 1.\n Cox-Snell residuals do not account for censored observations.\n One drawback to the Cox-Snell residuals is that they don’t provide much insight into why the model’s assumptions are violated。\n   Martingale Residuals4 \\(\\hat{m}_{i} = d_i - \\hat{\\Lambda}_i(t_i)\\)\n Martingale residuals can be used to assess the true functional form of a particular covariate (Thernau et al. (1990)).\n Positive values mean that the patient died sooner than expected (according to the model); negative values mean that the patient lived longer than expected (or were censored).\n Martingale residuals are very useful and can be used for many of the usual purposes that we use residuals for in other models (identifying outliers, choosing a functional form for the covariate, etc.)\n However, the primary drawback to the martingale residual is its clear asymmetry (its upper bound is 1, but it has no lowerbound)\n   Outliers Identification  Overall: plot deviance residual vs time.\n Variable Specific:plot dfBetas vs time.\n DFBetas Measure of how much an observation has effected the estimate of a regression coefficient (there is one DFBETA for each regression coefficient, including the intercept). Values larger than 2/sqrt(n) in absolute value are considered highly influential.\n   Linearity Visual  plot deviance residual vs continuous variable   Predictive Power Harrell’s C-index (concordance index)5:\nThe intuition behind Harrell’s C-index is as follows. For patient i, our risk model assigns a risk score \\(\\eta_i\\) . If our risk model is any good, patients who had shorter times-to-disease should have higher risk scores. Boiling this intuition down to two patients: the patient with the higher risk score should have a shorter time-to-disease.\nWe can compute the C-index in the following way: For every pair of patients i and j (with i \\(\\neq\\) j), look at their risk scores and times-to-event.\nBoth i and j are not censored If \\(T_j\\) \u0026lt; \\(T_i\\) and \\(\\eta_j \u0026lt; \\eta_i\\) =\u0026gt; discordant pair\nIf \\(T_j\\) \u0026lt; \\(T_i\\) and \\(\\eta_j \u0026gt; \\eta_i\\) =\u0026gt; concordant pair\ndiscordant pair:\n Both i and j are censored We don’t consider this pair in this situation.\n i is censored and j is not censored If \\(T_j\\) \u0026lt; \\(T_i\\) and \\(\\eta_i \u0026gt; \\eta_j\\) =\u0026gt; discordant pair\nIf \\(T_j\\) \u0026lt; \\(T_i\\) and \\(\\eta_i \u0026lt; \\eta_j\\) =\u0026gt; concordant pair\nIf \\(T_j\\) \u0026gt; \\(T_i\\) then we don’t know who got disease first, we don’t consider this pair in this situation.\nc = #concordant_pairs / #concordant_pairs + #discordant_pairs\nValues of c near 0.5 indicate that the risk score predictions are no better than a coin flip in determining which patient will live longer.\nValues near 1 indicate that the risk scores are good at determining which of two patients will have the disease first.\nValues near 0 means that the risk scores are worse than a coin flip: you might be better off concluding the opposite of what the risk scores tell you.\n    https://slideplayer.com/slide/14000214/↩\n https://stats.idre.ucla.edu/other/examples/asa2/testing-the-proportional-hazard-assumption-in-cox-models/↩\n https://stats.idre.ucla.edu/other/examples/asa2/testing-the-proportional-hazard-assumption-in-cox-models/↩\n https://myweb.uiowa.edu/pbreheny/7210/f15/notes/11-10.pdf↩\n https://statisticaloddsandends.wordpress.com/2019/10/26/what-is-harrells-c-index/↩\n   ","date":1582237969,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582237969,"objectID":"eee36e7d8b228d2173bed583bbf873bd","permalink":"/post/note12-cox-model-diagnostics/","publishdate":"2020-02-20T16:32:49-06:00","relpermalink":"/post/note12-cox-model-diagnostics/","section":"post","summary":"Testing the Proportional Hazard Assumptions Schoenfeld Residuals1  Instead of a single residual for each individual, there is a separate residual for each individual for each covariate.\n Schonefeld residuals are not defined for censored individuals.\n  \\(Schonefeld residual = x_{ik} - \\sum_{i=1}^{j\\in R(t_i)} x_{kj}P_j\\) - The Schoenfeld residual is the covariate-value, \\(x_{ik}\\), for the person i who actually died at time \\(t_i\\) minus the expected value of the covariate for the risk set at \\(t_i\\).","tags":["bds722","bds722note","cox-model-diagnostics"],"title":"Note12 Cox Model Diagnostics","type":"post"},{"authors":[],"categories":[],"content":" Cox Models = Propotional Hazards Models\nModeling the Hazard Function1  Survival analysis methods, such as proportional hazards regression differ from logistic regression by assessing a rate instead of a proportion.\n Proportional hazards regression, also called Cox regression, models the incidence or hazard rate, the number of new cases of disease per population at-risk per unit time. If the outcome is death, this is the mortality rate.\n The hazard function is the probability that if a person survives to t, they will experience the event in the next instant.\n  \\(\\lambda (t|X_{1i}, X_{2i}, ..., X_{ki}) = \\lambda _0 (t)exp(\\beta_1X_{1i} + \\beta_2X_{2i} + ... +\\beta_kX_{ki})\\)\n\\(log(\\frac{\\lambda (t|X_{1i}, X_{2i}, ..., X_{ki})}{\\lambda _0 (t)}) = exp(\\beta_1X_{1i} + \\beta_2X_{2i} + ... +\\beta_kX_{ki})\\)\n \\(\\lambda (t|X_{1i}, X_{2i}, ..., X_{ki}):\\) this is the the hazard function for the ith person at time t,i=1,2,⋯,n.\n \\(\\lambda _0 (t):\\) this is the baseline hazard function at time t, which is analogous to the intercept term in a multiple regression or logistic regression model. And it is unknown.\n \\(log(\\frac{\\lambda (t|X_{1i}, X_{2i}, ..., X_{ki})}{\\lambda _0 (t)}):\\) this is the log of hazard ratio. The ratio of hazard functions can be considered a ratio of risk functions.\n There is no particular probability model selected to represent the survival times.\n But there is a big assumption: the hazard for any individual is a fixed proportion of the hazard for any other individual. (i.e., proportional hazards). Notice if \\(λ_0(t)\\) is the hazard function for a subject with all the predictor values equal to zero and \\(λ_1(t)\\) is the hazard function for a subject with other values for the predictor variables, then the hazard ratio depends only on the predictor variables and not on time t. This assumption means if a covariate doubles the risk of the event on day one, it also doubles the risk of the event on any other day.\n   Output Interpretation  Hazard Ratio: similar with odds ratio.\n Continuous: A unit increase in X is associated with a YY% increase/decrease in the hazard of (event) on average.\n Binary: On average, A has YY% higher/lower hazard of (event) compared to B.\n    https://online.stat.psu.edu/stat507/node/81/↩\n   ","date":1582226492,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582226492,"objectID":"c05bdbb1f80d55dab2481b34dcd92351","permalink":"/post/note11-cox-models/","publishdate":"2020-02-20T13:21:32-06:00","relpermalink":"/post/note11-cox-models/","section":"post","summary":"Cox Models = Propotional Hazards Models\nModeling the Hazard Function1  Survival analysis methods, such as proportional hazards regression differ from logistic regression by assessing a rate instead of a proportion.\n Proportional hazards regression, also called Cox regression, models the incidence or hazard rate, the number of new cases of disease per population at-risk per unit time. If the outcome is death, this is the mortality rate.\n The hazard function is the probability that if a person survives to t, they will experience the event in the next instant.","tags":["bds722","bds722note","cox-models"],"title":"Note11 Cox Models","type":"post"},{"authors":[],"categories":[],"content":" Understanding K-M Curve   the K-M survival curve is defined as the probability of surviving in a given length of time while considering time in many small intervals.\n interval: the horizontal line in K-M graph.\nserial time duration contains two types of duration:\n  known survival(event occured)\n unknown survival(censored)\nand only 1 are called interval in K-M graph, 2 are indicated on K-M curve as tick marks.   vertical distance between horizontals: illustrate the change in cumulative survival probablity as the curve advances.\n K-M curve are not smooth function, but rather step-wise estimates.\n In K-M curve, the Y axis stands for cumulative surviving probability.\n cumulative surviving prob = \\(\\Pi (interval survival rate)\\), each horizontal line as an interval survival rate,\ninterval survival rate = \\(\\frac{total-number-of-current-alive-people}{total-number-of-alive-people- in-previous-horizontal-line-excluding-censored-people}\\)\n censoring has an effect on the survival rate.\n censoring removes the subjects from the denominator.\n K-M Estimation is a non-parametric estimation of survival function.\n when no event times are censored, a non-parametric estimator of S(T) is \\(1-F_{n}(t)\\), where \\(F_{n}(t)\\) is the empirical cumulative distribution function.\n when some observations are censored, we can estimate S(t) using KM estimation.\n  Let: \\(n_i\\) be number of objects at risk of dying at time \\(t_i\\)\n\\(d_i\\) be number of events that occur at time \\(t_i\\)\n KM estimator  \\(\\hat{S}(t) = \\prod_{k=1}^i \\frac{n_k - d_k}{n_k}\\)\n Basic tests for difference in survival curves Want to test:\n\\(H_0: S_1(t) = S_2(t)\\)\n\\(H_1: S_1(t) \\neq S_2(t)\\)\nmydat \u0026lt;- matrix(c(\u0026#39;d1i\u0026#39;,\u0026#39;d2i\u0026#39;,\u0026#39;n1i-d1i\u0026#39;,\u0026#39;n2i-d2i\u0026#39;),ncol = 2, byrow = T) colnames(mydat) \u0026lt;- c(\u0026#39;Group 1\u0026#39;, \u0026#39;Group 2\u0026#39;) rownames(mydat) \u0026lt;- c(\u0026#39;Died\u0026#39;,\u0026#39;Survived\u0026#39;) mydat ## Group 1 Group 2 ## Died \u0026quot;d1i\u0026quot; \u0026quot;d2i\u0026quot; ## Survived \u0026quot;n1i-d1i\u0026quot; \u0026quot;n2i-d2i\u0026quot; Test Statistic:\n\\(Q = \\frac{(\\sum w_i (d_{1i} - \\hat{e}_{1i}))^2}{\\sum w_i ^2 \\hat{v}_{1i}}\\) where:\n\\(\\hat{e}_{1i} = \\frac{n_{1i}d_{1i}}{n_{i}}\\)\n\\(\\hat{v}_{1i} = \\frac{n_{1i}n_{2i}d_i (n_i - d_i)}{n_i ^2 (n_i - 1)}\\)\n\\(w_i =\\) some weighting functing\nTests:\nLog-rank: \\(w_i = 1\\)\nWilcoxon: \\(w_i = n_i\\)\nTarone-ware: \\(w_i = \\sqrt{n_i}\\)\nPeto-Peto: \\(w_i = \\prod_{i=1}^T (\\frac{1-\\hat{e}_i}{n_i + 1})\\)\nReference:\nRich, J. T., Neely, J. G., Paniello, R. C., Voelker, C. C., Nussenbaum, B., \u0026amp; Wang, E. W. (2010). A practical guide to understanding Kaplan-Meier curves. Otolaryngology–head and neck surgery : official journal of American Academy of Otolaryngology-Head and Neck Surgery, 143(3), 331–336. https://doi.org/10.1016/j.otohns.2010.05.007\n ","date":1582084435,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582084435,"objectID":"636e9f30fcf19eef96843be5102ae1df","permalink":"/post/note10-kaplan-meier-estimation/","publishdate":"2020-02-18T21:53:55-06:00","relpermalink":"/post/note10-kaplan-meier-estimation/","section":"post","summary":"Understanding K-M Curve   the K-M survival curve is defined as the probability of surviving in a given length of time while considering time in many small intervals.\n interval: the horizontal line in K-M graph.\nserial time duration contains two types of duration:\n  known survival(event occured)\n unknown survival(censored)\nand only 1 are called interval in K-M graph, 2 are indicated on K-M curve as tick marks.","tags":["bds722","bds722note","km-estimation"],"title":"Note10 Kaplan Meier Estimation","type":"post"},{"authors":[],"categories":[],"content":" Time to Event Data  Keywords: Failure, Lifetime Data, Time-event, Time-to-event-data\n  Outcome: Time until some events occures -\u0026gt; survival analysis.\n In order to measure survival time, need a begining(t=0) and end(t = T).\n t=0 is when the clock starts.\n t=T is when the event occurs(or censored) + clock stops.\n Between this two times(t - T), the subject is said to be “at risk” for the event.\n Survival time T is the difference between these two points.\n Usually measured in days, months or years.\n  Question: Time is continuous, why not using linear regression?\nAnswer: Censoring! If no censoring, you can use it!\n Censoring Right Censored(by for the most common)  known survival time is greater than some value: T \u0026gt; a   Left Censored(rare)  known survival time is less than some value: T \u0026lt; b   Interval Censored  known survival time is within some interval: a \u0026lt; T \u0026lt; b    Functions of Survival Time  PDF: f(t)\n CDF: F(t) = P(T\u0026lt;t)\n Survival Function S(t): S(t) = P(T\u0026gt;t) = 1 - F(t), this is the prob that people survived passed t.\n Unconditional probability failure rate f(t): \\(f(t) = \\lim_{\\Delta t\\to\\ 0}\\frac{P(t \\leq T \\leq t + \\Delta t)}{\\Delta t}\\)\n  But, subjects who have died before time t are no longer at risk during (t, t + t).\nSo it make sense to think of a conditional failure rate.\n Conditional failure rate: risk of an event occuring at time t, given the subject has survived to time t.\n Hazard Function h(t)\n  \\(h(t) = \\lim_{\\Delta t\\to\\ 0}\\frac{P(t \\leq T \\leq t + \\Delta t | T \\geq t)}{\\Delta t}\\), h(t) \u0026gt; 0, h(t) is not a probability. The hazard function, h(t), is the instantaneous rate at which events occur, given no previous events.\n Cumulative Hazard Function H(t)  \\(H(t) = \\int\\limits_{0}^{t} h(u) du\\)\n Relationships between S(t), H(t), h(t), f(t)  \\(h(t) = - \\frac{\\partial log(S(t))}{\\partial t} = \\frac{f(t)}{S(t)}\\)\n\\(H(t) = -log(S(t))\\)\n\\(S(t) = exp(-H(t))\\)\n\\(f(t) = - \\frac{\\partial S(t)}{\\partial t}\\)\n Comment for Survival Distributions     Distribution h(t) S(t) f(t)    Exponential \\(\\lambda\\) \\(exp(-\\lambda t)\\) \\(\\lambda exp(-\\lambda t)\\)  Weibull $t^{-1} $ \\(exp(-\\lambda t^\\alpha)\\) \\(\\alpha \\lambda t^{\\alpha -1}exp(-\\lambda t^\\alpha)\\)  Gompertze \\(\\theta exp(\\alpha t)\\) \\(exp(\\frac{\\theta}{\\alpha}(1-exp(\\alpha t)))\\) \\(\\theta exp(\\alpha t)exp(\\frac{\\theta}{\\alpha}(1-exp(\\alpha t)))\\)    Others: Gamma, lognormal, log-logistic, inverse gaussian, Pareto\n ","date":1582068824,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582068824,"objectID":"0ea78453daedc6f03606e5c463c3931b","permalink":"/post/note9-intro-to-survival-analysis/","publishdate":"2020-02-18T17:33:44-06:00","relpermalink":"/post/note9-intro-to-survival-analysis/","section":"post","summary":"Time to Event Data  Keywords: Failure, Lifetime Data, Time-event, Time-to-event-data\n  Outcome: Time until some events occures -\u0026gt; survival analysis.\n In order to measure survival time, need a begining(t=0) and end(t = T).\n t=0 is when the clock starts.\n t=T is when the event occurs(or censored) + clock stops.\n Between this two times(t - T), the subject is said to be “at risk” for the event.","tags":["bds722","bds722note","survival-analysis"],"title":"Note9 Intro to Survival Analysis","type":"post"},{"authors":["Mengna Zhang"],"categories":null,"content":" Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic  Academic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click  PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions?  Ask\n Documentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"},{"authors":["Mengna Zhang","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"publication"},{"authors":["Mengna Zhang","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example conference paper","type":"publication"}]